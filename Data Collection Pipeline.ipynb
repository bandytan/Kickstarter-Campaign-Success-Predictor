{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63dfa3aa",
   "metadata": {},
   "source": [
    "# Data Collection Pipeline\n",
    "\n",
    "- Creates **data/kickstart_data_merged_with_empty.csv**\n",
    "- Next file to run: Data Cleaning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158cdb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e776153",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6aca66",
   "metadata": {},
   "source": [
    "## Import data from scraping from Kickstarter GraphQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e87ed02",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ivankoh/Downloads/bt4222 scraped combined/0_44134_story.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ivankoh/Downloads/bt4222 scraped combined/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0_44134_story.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     story_list1 \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(start \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0_44134_risk.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ivankoh/Downloads/bt4222 scraped combined/0_44134_story.json'"
     ]
    }
   ],
   "source": [
    "start = \"data/\"\n",
    "\n",
    "with open(start + \"0_44134_story.json\", \"r\") as f:\n",
    "    story_list1 = json.load(f)\n",
    "with open(start + \"0_44134_risk.json\", \"r\") as f:\n",
    "    risk_list1 = json.load(f)\n",
    "with open(start + \"44135_81404_story.json\", \"r\") as f:\n",
    "    story_list2 = json.load(f)\n",
    "with open(start + \"44135_81404_risk.json\", \"r\") as f:\n",
    "    risk_list2 = json.load(f)\n",
    "with open(start + \"81404_122106_story.json\", \"r\") as f:\n",
    "    story_list3 = json.load(f)\n",
    "with open(start + \"81404_122106_risk.json\", \"r\") as f:\n",
    "    risk_list3 = json.load(f)\n",
    "with open(start + \"162808_203510_story.json\", \"r\") as f:\n",
    "    story_list5 = json.load(f)\n",
    "with open(start + \"162808_203510_risk.json\", \"r\") as f:\n",
    "    risk_list5 = json.load(f)\n",
    "\n",
    "story_list = story_list1 + story_list2\n",
    "risk_list = risk_list1 + risk_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb50bc",
   "metadata": {},
   "source": [
    "## Import data from Web Robots (Web scraping service with public datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_df = pd.read_csv(\"data/web_robots.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54fa95",
   "metadata": {},
   "source": [
    "## Import data from WebScraper.io Scraping (Web scraping service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81690aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"data/\"\n",
    "webscraper_df = pd.read_excel(data + \"kickstarter_by_categories.xlsx\")\n",
    "graph_df1 = pd.read_csv(data + \"bandy.csv\")\n",
    "graph_df2 = pd.read_csv(data + \"merged_ivan_valentin.csv\")\n",
    "graph_df3 = pd.read_csv(data + \"df1_radell.csv\")\n",
    "\n",
    "graph_df2_mapping = graph_df2[['ivan_index', 'valentin_index']]\n",
    "print(len(webscraper_df))\n",
    "webscraper_df = webscraper_df.drop_duplicates('Link-href')\n",
    "print(len(webscraper_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e75a0",
   "metadata": {},
   "source": [
    "# Data Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e70d3",
   "metadata": {},
   "source": [
    "## Merge GraphQL Data (Story + Risk) with Webrobots Dataset to get Dataset X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Webrobots + GraphQL datasets\n",
    "\n",
    "# graph1 + webrobots\n",
    "df_robot_join = robot_df.merge(graph_df1, on=\"id\", how=\"left\")\n",
    "df_robot_join['final_index'] = range(len(df_robot_join))\n",
    "\n",
    "if len(story_list) == len(risk_list):\n",
    "    for i, item in enumerate(story_list):\n",
    "        df_robot_join.loc[df_robot_join['final_index'] == i, ['story']] = story_list[i]\n",
    "        df_robot_join.loc[df_robot_join['final_index'] == i, ['risk']] = risk_list[i]\n",
    "else:\n",
    "    print(\"check story and risk list\")\n",
    "\n",
    "# graph3 + webrobots\n",
    "graph_df3['graph_df3_index'] = range(len(graph_df3))\n",
    "graph_df3 = graph_df3[(graph_df3['graph_df3_index'] >= 162808) & (graph_df3['graph_df3_index'] < 203510)]\n",
    "graph_df3 = graph_df3[['graph_df3_index', 'id']]\n",
    "graph_df3_range = df_robot_join[(df_robot_join['final_index'] >= 162808) & (df_robot_join['final_index'] < 203510)]\n",
    "graph_df3_merged = graph_df3_range.merge(graph_df3,on='id',how='inner')\n",
    "\n",
    "graph_df3_mapping = graph_df3_merged[['final_index', 'graph_df3_index']].sort_values('graph_df3_index')\n",
    "base_num = 162808\n",
    "\n",
    "for idx, row in graph_df3_mapping.iterrows():\n",
    "    df_robot_join.loc[df_robot_join['final_index'] == row['final_index'], ['story']] = story_list5[row['graph_df3_index'] - base_num]\n",
    "    df_robot_join.loc[df_robot_join['final_index'] == row['final_index'], ['risk']] = risk_list5[row['graph_df3_index'] - base_num]\n",
    "\n",
    "# graph2 + webrobots    \n",
    "graph_df2_mapping = graph_df2_mapping.sort_values('valentin_index')\n",
    "base_num = 81404\n",
    "\n",
    "for idx, row in graph_df2_mapping.iterrows():\n",
    "    df_robot_join.loc[df_robot_join['final_index'] == row['ivan_index'], ['story']] = story_list3[row['valentin_index'] - base_num]\n",
    "    df_robot_join.loc[df_robot_join['final_index'] == row['ivan_index'], ['risk']] = risk_list3[row['valentin_index'] - base_num]\n",
    "\n",
    "# Get dataset_x: Exclude graphql data that could not fully merge with webrobots\n",
    "dataset_x = df_robot_join[pd.notnull(df_robot_join['story'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f6b0e",
   "metadata": {},
   "source": [
    "## Merge Final GraphQL Dataset (Story + Risk) with Webscraper Dataset (Rewards)\n",
    "\n",
    "In the proposal, the Final GraphQL Dataset is known as Dataset X, and the Webscraper Dataset is known as Dataset Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb098830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing join key \n",
    "dataset_y = webscraper_df\n",
    "dataset_y['Link-href'] = dataset_y['Link-href'].apply(lambda x: str(x)[:str(x).find(\"?ref\")])\n",
    "dataset_x['main_url'] = dataset_x['main_url'].apply(lambda x: str(x)[:str(x).find(\"?ref\")])\n",
    "combined_dataset = dataset_y.merge(dataset_x, left_on=\"Link-href\", right_on=\"main_url\", how=\"left\")\n",
    "\n",
    "## FILL IN MISSING DATA HERE\n",
    "combined_dataset.to_csv(\"data/kickstart_data_merged_with_empty.csv\")\n",
    "\n",
    "# Exclude data in dataset X that could not fully merge with dataset Y\n",
    "final_combined_dataset = combined_dataset[pd.notnull(combined_dataset['story'])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(combined_dataset))\n",
    "print(len(final_combined_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08932a0",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_dataset.to_csv(\"data/kickstarter_data_merged.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (bt4222)",
   "language": "python",
   "name": "bt4222"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
