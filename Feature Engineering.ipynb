{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcff7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b667a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Kickstarter_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a6a68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1404\n",
       "0     728\n",
       "Name: has_video, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'video' to a binary categorical variable\n",
    "df['video'].value_counts()\n",
    "df['has_video'] = df['video'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "df['has_video'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6f8c5",
   "metadata": {},
   "source": [
    "NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1445895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning for: rewards, description, description story, description risks\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    def process_rewards(corpus):\n",
    "    \n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row_processed = \"\"\n",
    "            row = row.replace(\"\\\\n\", \" \")\n",
    "            row = ast.literal_eval(row)\n",
    "\n",
    "            for dict in row:\n",
    "                row_processed += dict['rewards'].lower() + ' '\n",
    "            \n",
    "            \n",
    "            row_processed = row_processed.replace(\"//\",'')\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed)\n",
    "\n",
    "        return corpus_processed\n",
    "    \n",
    "    def process_description_story(corpus):\n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row = str(row)\n",
    "            row_processed = row.replace(\"\\r\", \" \" )\n",
    "            row_processed = row_processed.replace(\"\\n\", \" \" )\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed)\n",
    "\n",
    "        return corpus_processed\n",
    "\n",
    "    df[\"rewards_processed\"] = process_rewards(df[\"rewards\"])\n",
    "    df[\"description_processed\"] = process_description_story(df[\"description\"])\n",
    "    df[\"description_story_processed\"] = process_description_story(df[\"description_story\"])\n",
    "    df[\"description_risks_processed\"] = process_description_story(df[\"description_risks\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95c3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizeTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62c9ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nlp_features(df):\n",
    "    # Rewards\n",
    "\n",
    "    vect_rewards = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    rewards_processed = pd.Series(df[\"rewards_processed\"])\n",
    "    tfidf_fit_rewards = vect_rewards.fit(rewards_processed)\n",
    "    rewards_tfidf_array = tfidf_fit_rewards.transform(rewards_processed).toarray()\n",
    "    df = pd.merge(df, pd.DataFrame(rewards_tfidf_array), left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965ebbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "generate_nlp_features(clean_text(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2d5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77238a471535228e8cd55a3ca9e771a69c6c0bc66c44a56c972f9554a4042742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
