{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfcff7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "import pyLDAvis.gensim_models\n",
    "import en_core_web_md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b667a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/kickstarter_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30a6a68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1015\n",
       "0     542\n",
       "Name: has_video, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'video' to a binary categorical variable\n",
    "df['video'].value_counts()\n",
    "df['has_video'] = df['video'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "df['has_video'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6f8c5",
   "metadata": {},
   "source": [
    "## NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1445895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning for: rewards, description, description story, description risks\n",
    "\n",
    "def clean_text(df):\n",
    "    def process_rewards(corpus):\n",
    "    \n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row_processed = \"\"\n",
    "            row = row.replace(\"\\\\n\", \" \")\n",
    "            row = ast.literal_eval(row)\n",
    "\n",
    "            for dict in row:\n",
    "                row_processed += dict['rewards'].lower() + ' '\n",
    "            \n",
    "            \n",
    "            row_processed = row_processed.replace(\"//\",'')\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed)\n",
    "\n",
    "        return corpus_processed\n",
    "    \n",
    "    def process_description_story(corpus):\n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row = str(row)\n",
    "            row_processed = row.replace(\"\\r\", \" \" )\n",
    "            row_processed = row_processed.replace(\"\\n\", \" \" )\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed if not pd.isnull(row_processed) else \"\") # handle NA\n",
    "\n",
    "        return corpus_processed\n",
    "\n",
    "    df[\"rewards_processed\"] = process_rewards(df[\"rewards\"])\n",
    "    df[\"description_processed\"] = process_description_story(df[\"description\"])\n",
    "    df[\"description_story_processed\"] = process_description_story(df[\"description_story\"])\n",
    "    df[\"description_risks_processed\"] = process_description_story(df[\"description_risks\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f95c3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizeTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62c9ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nlp_features(df):\n",
    "    \n",
    "    # Rewards\n",
    "\n",
    "    vect_rewards = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    rewards_processed = pd.Series(df[\"rewards_processed\"])\n",
    "    tfidf_fit_rewards = vect_rewards.fit(rewards_processed)\n",
    "    rewards_tfidf_array = tfidf_fit_rewards.transform(rewards_processed).toarray()\n",
    "    rewards_tfidf_df = pd.DataFrame(rewards_tfidf_array)\n",
    "    rewards_tfidf_df.columns = list(map(lambda x : \"rewards_\" + str(x), rewards_tfidf_df.columns))\n",
    "    df = pd.merge(df, rewards_tfidf_df , left_index=True, right_index=True)\n",
    "    \n",
    "\n",
    "    # Description\n",
    "\n",
    "    vect_description = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    description = pd.Series(df[\"description_processed\"])\n",
    "    tfidf_fit_description = vect_description.fit(description)\n",
    "    description_tfidf_array = tfidf_fit_description.transform(description).toarray()\n",
    "    description_tfidf_df = pd.DataFrame(description_tfidf_array)\n",
    "    description_tfidf_df.columns = list(map(lambda x : \"description_\" + str(x), description_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Description Story\n",
    "\n",
    "    vect_description_story = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "    \n",
    "    description_story_processed = pd.Series(df[\"description_story_processed\"])\n",
    "    tfidf_fit_description_story_processed = vect_description_story.fit(description_story_processed)\n",
    "    description_story_processed_tfidf_array = tfidf_fit_description_story_processed.transform(description_story_processed).toarray()\n",
    "    description_story_tfidf_df = pd.DataFrame(description_story_processed_tfidf_array)\n",
    "    description_story_tfidf_df.columns = list(map(lambda x : \"description_story_\" + str(x), description_story_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_story_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Description Risks\n",
    "\n",
    "    vect_description_risks = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    description_risks_processed = pd.Series(df[\"description_risks_processed\"])\n",
    "    tfidf_fit_description_risks_processed = vect_description_risks.fit(description_risks_processed)\n",
    "    description_risks_processed_tfidf_array = tfidf_fit_description_risks_processed.transform(description_risks_processed).toarray()\n",
    "    description_risks_tfidf_df = pd.DataFrame(description_risks_processed_tfidf_array)\n",
    "    description_risks_tfidf_df.columns = list(map(lambda x : \"description_risks_\" + str(x), description_risks_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_risks_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dae2d5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "name\n",
      "description\n",
      "description_story\n",
      "description_risks\n",
      "rewards\n",
      "category\n",
      "pledged\n",
      "goal\n",
      "deadline\n",
      "location\n",
      "state\n",
      "faq_count\n",
      "update_count\n",
      "backers_count\n",
      "spotlight\n",
      "staff_pick\n",
      "video\n",
      "launched_at\n",
      "has_video\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883c447",
   "metadata": {},
   "source": [
    "## Rewards Features\n",
    "\n",
    "<font color=\"red\"><strong>I dont think list can be a input to ML</strong></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5880f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rewards_tiers(df):\n",
    "    df[\"reward_tiers\"] = df[\"rewards\"].apply(lambda x : len(ast.literal_eval(x)))\n",
    "    df = move_reward_tiers(df)\n",
    "    return df\n",
    "\n",
    "def create_all_reward_amount(df):\n",
    "    df[\"all_reward_amount\"] = np.empty((len(df), 0)).tolist()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        all_reward_amount = []\n",
    "        dict_list = ast.literal_eval(df.iloc[i, 5]) # Converts rewards column into dictionary\n",
    "\n",
    "        for dict in dict_list:\n",
    "            values_string = str(dict.values())\n",
    "            reward_title = re.search(r\"Pledge S\\$ \\d{1,3}(,\\d{1,3})? or more\", values_string) # Search for all reward titles\n",
    "\n",
    "            if reward_title is not None:\n",
    "                reward_amount = re.search(r\"\\d{1,3}(,\\d{1,3})?\", reward_title.group()) # Search for only the digits in reward amount\n",
    "                if reward_amount is not None:\n",
    "                    all_reward_amount.append(reward_amount.group())\n",
    "            else:\n",
    "                all_reward_amount.append(0) # If no reward title is found, add 0\n",
    "        df[\"all_reward_amount\"][i] = all_reward_amount\n",
    "    df = move_all_reward_amount(df)\n",
    "    return df\n",
    "\n",
    "# Rearange reward_tiers column to the right of rewards\n",
    "def move_reward_tiers(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[:6] + [cols[-1]] + cols[6:-1]\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "# Rearange all_reward_amount column to the right of reward_tiers\n",
    "def move_all_reward_amount(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[:7] + [cols[-1]] + cols[7:-1]\n",
    "    df = df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e88b48",
   "metadata": {},
   "source": [
    "## Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2008d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].\n",
    "'''\n",
    "def generate_sentiment_features(df):\n",
    "    df = df.dropna(subset=['description_story_processed', 'description_risks_processed', 'description_processed', 'rewards_processed']) # NOTE: put at top with other dropnas from other features?\n",
    "    df[\"description_story_polarity\"] = df[\"description_story_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_story_subjectivity\"] = df[\"description_story_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"description_polarity\"] = df[\"description_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_subjectivity\"] = df[\"description_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"description_risks_polarity\"] = df[\"description_risks_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_risks_subjectivity\"] = df[\"description_risks_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"rewards_polarity\"] = df[\"rewards_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"rewards_subjectivity\"] = df[\"rewards_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb8913eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_count_features(df): # omitted description due to word limit, word count likely similar for all projects\n",
    "    df['description_story_word_count'] = df[\"description_story_processed\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['description_risks_word_count'] = df[\"description_risks_processed\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['rewards_word_count'] = df[\"rewards_processed\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb3dab",
   "metadata": {},
   "source": [
    "## One-hot Encoding of Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e87cf",
   "metadata": {},
   "source": [
    "when to drop one of the OHE columns: \n",
    "https://stats.stackexchange.com/questions/231285/dropping-one-of-the-columns-when-using-one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d13b9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_transform(enc, col, df):\n",
    "    transformed = enc.transform(df[col].to_numpy().reshape(-1, 1))\n",
    "    #Create a Pandas DataFrame of the hot encoded column\n",
    "    ohe_df = pd.DataFrame(transformed, columns=enc.get_feature_names_out())\n",
    "    #concat with original data\n",
    "    data = pd.concat([df, ohe_df], axis=1).drop([col], axis=1)\n",
    "    return data\n",
    "\n",
    "def ohe_fit(col, df):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    enc.fit(df[col].unique().reshape(-1, 1))\n",
    "    return (ohe_transform(enc, col, df), enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf488355",
   "metadata": {},
   "source": [
    "hold out on topic modelling first because it is unsupervised algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe67541",
   "metadata": {},
   "source": [
    "## Combine all feature generating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35d0b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_25536/2749083875.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"all_reward_amount\"][i] = all_reward_amount\n"
     ]
    }
   ],
   "source": [
    "result = clean_text(df)\n",
    "result = generate_nlp_features(result)\n",
    "result = create_rewards_tiers(result)\n",
    "result = create_all_reward_amount(result)\n",
    "result = generate_sentiment_features(result)\n",
    "result = generate_word_count_features(result)\n",
    "result, category_encoder = ohe_fit('category', result) #use encoder to fit train data\n",
    "result, location_encoder = ohe_fit('location', result) #use encoder to fit train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af08fa",
   "metadata": {},
   "source": [
    "## Apply same feature engineering on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "906cb5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_25536/2749083875.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"all_reward_amount\"][i] = all_reward_amount\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"./data/kickstarter_test.csv\")\n",
    "\n",
    "test = clean_text(test)\n",
    "test = generate_nlp_features(test)\n",
    "test = create_rewards_tiers(test)\n",
    "test = create_all_reward_amount(test)\n",
    "test = generate_sentiment_features(test)\n",
    "test = generate_word_count_features(test)\n",
    "\n",
    "#encoders\n",
    "test = ohe_transform(category_encoder, 'category', test)\n",
    "test = ohe_transform(location_encoder, 'location', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bad7307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_story</th>\n",
       "      <th>description_risks</th>\n",
       "      <th>rewards</th>\n",
       "      <th>reward_tiers</th>\n",
       "      <th>all_reward_amount</th>\n",
       "      <th>pledged</th>\n",
       "      <th>goal</th>\n",
       "      <th>...</th>\n",
       "      <th>x0_technology/web</th>\n",
       "      <th>x0_theater/comedy</th>\n",
       "      <th>x0_theater/experimental</th>\n",
       "      <th>x0_theater/immersive</th>\n",
       "      <th>x0_theater/musical</th>\n",
       "      <th>x0_theater/plays</th>\n",
       "      <th>x0_Orchard, Singapore</th>\n",
       "      <th>x0_Queenstown, Singapore</th>\n",
       "      <th>x0_Sembawang, Singapore</th>\n",
       "      <th>x0_Singapore, Singapore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505555120</td>\n",
       "      <td>Vertica Green - Mission to make cities green!</td>\n",
       "      <td>Vertical Gardens set up at schools that are fu...</td>\n",
       "      <td>StoryThat’s me (on the left). Anything related...</td>\n",
       "      <td>The majority of the cost for the project is up...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 5 or more\\n\\nAbout US$...</td>\n",
       "      <td>3</td>\n",
       "      <td>[5, 25, 200]</td>\n",
       "      <td>89.0</td>\n",
       "      <td>195000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1161363320</td>\n",
       "      <td>The Trojan</td>\n",
       "      <td>A crossbreed of armies &amp; animals, innocence &amp; ...</td>\n",
       "      <td>StoryWhat's The Deal?\\n\"All warfare is based o...</td>\n",
       "      <td>Logistic is one of the main challenges in maki...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 2 or more\\n\\nAbout $2\\...</td>\n",
       "      <td>11</td>\n",
       "      <td>[2, 12, 23, 23, 30, 50, 70, 90, 25, 45, 120]</td>\n",
       "      <td>410.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152317277</td>\n",
       "      <td>Plain Sight, a 4 Dimension production</td>\n",
       "      <td>an orphan boy must choose between future he dr...</td>\n",
       "      <td>StoryPlain Sight is a 10 minute short film sto...</td>\n",
       "      <td>Although we have the skills for each roles, di...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 5 or more\\n\\nAbout US$...</td>\n",
       "      <td>5</td>\n",
       "      <td>[5, 10, 50, 100, 500]</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1862517692</td>\n",
       "      <td>No Brainer #1-4 : A Solo Comix Anthology</td>\n",
       "      <td>The 48-page 4th issue comes with 2 big stories...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\nWhat is a No-Brainer?\\nNo...</td>\n",
       "      <td>The book is all done and ready for printing. I...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 3 or more\\n\\nAbout US$...</td>\n",
       "      <td>6</td>\n",
       "      <td>[3, 8, 14, 18, 75, 75]</td>\n",
       "      <td>5145.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960297971</td>\n",
       "      <td>Toast the Raccoon Cute Plushie Toys + Keychains</td>\n",
       "      <td>Chonky Raccoon named Toast! It's made into a h...</td>\n",
       "      <td>StoryMeet our chunky little bandit mask friend...</td>\n",
       "      <td>Challenges we faced are often lies on overall ...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 27 or more\\n\\nAbout $2...</td>\n",
       "      <td>9</td>\n",
       "      <td>[27, 36, 54, 61, 108, 121, 215, 54, 81]</td>\n",
       "      <td>70514.0</td>\n",
       "      <td>18000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2129055939</td>\n",
       "      <td>The Breakfast Project Singapore</td>\n",
       "      <td>Join a foodie (also a food technologist) who i...</td>\n",
       "      <td>StoryIf you are a health freak like me, you mi...</td>\n",
       "      <td>1st stage:- Meeting the volume requirement. \\n...</td>\n",
       "      <td>[{'rewards': 'Pledge AU$ 30 or more\\n\\nAbout $...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>299460577</td>\n",
       "      <td>Jellyfish - A Short Film Set In Borneo</td>\n",
       "      <td>Set in a beautiful remote fishing village, thi...</td>\n",
       "      <td>StoryBlossoming sexuality, teenage desire and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'rewards': \"Pledge $10 or more\\n\\nAbout $10\\...</td>\n",
       "      <td>8</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>753139167</td>\n",
       "      <td>Double Bass Speaker by UB+</td>\n",
       "      <td>Explosive bass and clarity from a portable spe...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>As the world is swept by Covid19 pandemic and ...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 209 or more\\n\\nAbout U...</td>\n",
       "      <td>13</td>\n",
       "      <td>[209, 246, 251, 279, 283, 323, 403, 419, 487, ...</td>\n",
       "      <td>34343.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>1922446531</td>\n",
       "      <td>Touge Shakai</td>\n",
       "      <td>A Touge Racing Simcade</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\nTouge Shakai (Mountain Pa...</td>\n",
       "      <td>The amount of content will be limited based on...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 3 or more\\n\\nAbout US$...</td>\n",
       "      <td>6</td>\n",
       "      <td>[3, 15, 30, 60, 100, 1,000]</td>\n",
       "      <td>7038.0</td>\n",
       "      <td>24000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>447415872</td>\n",
       "      <td>Brave Face for Men</td>\n",
       "      <td>The manliest way to cover up those pimples on ...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLet's say your a ...</td>\n",
       "      <td>We are confident that our manufacturer for the...</td>\n",
       "      <td>[{'rewards': \"Pledge $25 or more\\n\\nAbout $25\\...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>51.0</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 479 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                             name  \\\n",
       "0     505555120    Vertica Green - Mission to make cities green!   \n",
       "1    1161363320                                       The Trojan   \n",
       "2     152317277            Plain Sight, a 4 Dimension production   \n",
       "3    1862517692         No Brainer #1-4 : A Solo Comix Anthology   \n",
       "4    1960297971  Toast the Raccoon Cute Plushie Toys + Keychains   \n",
       "..          ...                                              ...   \n",
       "288  2129055939                  The Breakfast Project Singapore   \n",
       "289   299460577           Jellyfish - A Short Film Set In Borneo   \n",
       "290   753139167                       Double Bass Speaker by UB+   \n",
       "291  1922446531                                     Touge Shakai   \n",
       "292   447415872                               Brave Face for Men   \n",
       "\n",
       "                                           description  \\\n",
       "0    Vertical Gardens set up at schools that are fu...   \n",
       "1    A crossbreed of armies & animals, innocence & ...   \n",
       "2    an orphan boy must choose between future he dr...   \n",
       "3    The 48-page 4th issue comes with 2 big stories...   \n",
       "4    Chonky Raccoon named Toast! It's made into a h...   \n",
       "..                                                 ...   \n",
       "288  Join a foodie (also a food technologist) who i...   \n",
       "289  Set in a beautiful remote fishing village, thi...   \n",
       "290  Explosive bass and clarity from a portable spe...   \n",
       "291                             A Touge Racing Simcade   \n",
       "292  The manliest way to cover up those pimples on ...   \n",
       "\n",
       "                                     description_story  \\\n",
       "0    StoryThat’s me (on the left). Anything related...   \n",
       "1    StoryWhat's The Deal?\\n\"All warfare is based o...   \n",
       "2    StoryPlain Sight is a 10 minute short film sto...   \n",
       "3    Story\\n\\n\\n\\n\\n\\n\\n\\nWhat is a No-Brainer?\\nNo...   \n",
       "4    StoryMeet our chunky little bandit mask friend...   \n",
       "..                                                 ...   \n",
       "288  StoryIf you are a health freak like me, you mi...   \n",
       "289  StoryBlossoming sexuality, teenage desire and ...   \n",
       "290  Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "291  Story\\n\\n\\n\\n\\n\\n\\n\\nTouge Shakai (Mountain Pa...   \n",
       "292  Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLet's say your a ...   \n",
       "\n",
       "                                     description_risks  \\\n",
       "0    The majority of the cost for the project is up...   \n",
       "1    Logistic is one of the main challenges in maki...   \n",
       "2    Although we have the skills for each roles, di...   \n",
       "3    The book is all done and ready for printing. I...   \n",
       "4    Challenges we faced are often lies on overall ...   \n",
       "..                                                 ...   \n",
       "288  1st stage:- Meeting the volume requirement. \\n...   \n",
       "289                                                NaN   \n",
       "290  As the world is swept by Covid19 pandemic and ...   \n",
       "291  The amount of content will be limited based on...   \n",
       "292  We are confident that our manufacturer for the...   \n",
       "\n",
       "                                               rewards  reward_tiers  \\\n",
       "0    [{'rewards': 'Pledge S$ 5 or more\\n\\nAbout US$...             3   \n",
       "1    [{'rewards': 'Pledge S$ 2 or more\\n\\nAbout $2\\...            11   \n",
       "2    [{'rewards': 'Pledge S$ 5 or more\\n\\nAbout US$...             5   \n",
       "3    [{'rewards': 'Pledge S$ 3 or more\\n\\nAbout US$...             6   \n",
       "4    [{'rewards': 'Pledge S$ 27 or more\\n\\nAbout $2...             9   \n",
       "..                                                 ...           ...   \n",
       "288  [{'rewards': 'Pledge AU$ 30 or more\\n\\nAbout $...             2   \n",
       "289  [{'rewards': \"Pledge $10 or more\\n\\nAbout $10\\...             8   \n",
       "290  [{'rewards': 'Pledge S$ 209 or more\\n\\nAbout U...            13   \n",
       "291  [{'rewards': 'Pledge S$ 3 or more\\n\\nAbout US$...             6   \n",
       "292  [{'rewards': \"Pledge $25 or more\\n\\nAbout $25\\...             2   \n",
       "\n",
       "                                     all_reward_amount  pledged    goal  ...  \\\n",
       "0                                         [5, 25, 200]     89.0  195000  ...   \n",
       "1         [2, 12, 23, 23, 30, 50, 70, 90, 25, 45, 120]    410.0    8000  ...   \n",
       "2                                [5, 10, 50, 100, 500]    102.0    1000  ...   \n",
       "3                               [3, 8, 14, 18, 75, 75]   5145.0    4000  ...   \n",
       "4              [27, 36, 54, 61, 108, 121, 215, 54, 81]  70514.0   18000  ...   \n",
       "..                                                 ...      ...     ...  ...   \n",
       "288                                             [0, 0]     30.0    4000  ...   \n",
       "289                           [0, 0, 0, 0, 0, 0, 0, 0]   9200.0    8000  ...   \n",
       "290  [209, 246, 251, 279, 283, 323, 403, 419, 487, ...  34343.0   10000  ...   \n",
       "291                        [3, 15, 30, 60, 100, 1,000]   7038.0   24000  ...   \n",
       "292                                             [0, 0]     51.0   50000  ...   \n",
       "\n",
       "    x0_technology/web  x0_theater/comedy x0_theater/experimental  \\\n",
       "0                 0.0                0.0                     0.0   \n",
       "1                 0.0                0.0                     0.0   \n",
       "2                 0.0                0.0                     0.0   \n",
       "3                 0.0                0.0                     0.0   \n",
       "4                 0.0                0.0                     0.0   \n",
       "..                ...                ...                     ...   \n",
       "288               0.0                0.0                     0.0   \n",
       "289               0.0                0.0                     0.0   \n",
       "290               0.0                0.0                     0.0   \n",
       "291               0.0                0.0                     0.0   \n",
       "292               0.0                0.0                     0.0   \n",
       "\n",
       "     x0_theater/immersive  x0_theater/musical  x0_theater/plays  \\\n",
       "0                     0.0                 0.0               0.0   \n",
       "1                     0.0                 0.0               0.0   \n",
       "2                     0.0                 0.0               0.0   \n",
       "3                     0.0                 0.0               0.0   \n",
       "4                     0.0                 0.0               0.0   \n",
       "..                    ...                 ...               ...   \n",
       "288                   0.0                 0.0               0.0   \n",
       "289                   0.0                 0.0               0.0   \n",
       "290                   0.0                 0.0               0.0   \n",
       "291                   0.0                 0.0               0.0   \n",
       "292                   0.0                 0.0               0.0   \n",
       "\n",
       "     x0_Orchard, Singapore x0_Queenstown, Singapore x0_Sembawang, Singapore  \\\n",
       "0                      0.0                      0.0                     0.0   \n",
       "1                      0.0                      0.0                     0.0   \n",
       "2                      0.0                      0.0                     0.0   \n",
       "3                      0.0                      0.0                     0.0   \n",
       "4                      0.0                      0.0                     0.0   \n",
       "..                     ...                      ...                     ...   \n",
       "288                    0.0                      0.0                     0.0   \n",
       "289                    0.0                      0.0                     0.0   \n",
       "290                    0.0                      0.0                     0.0   \n",
       "291                    0.0                      0.0                     0.0   \n",
       "292                    0.0                      0.0                     0.0   \n",
       "\n",
       "    x0_Singapore, Singapore  \n",
       "0                       1.0  \n",
       "1                       1.0  \n",
       "2                       1.0  \n",
       "3                       1.0  \n",
       "4                       1.0  \n",
       "..                      ...  \n",
       "288                     1.0  \n",
       "289                     1.0  \n",
       "290                     1.0  \n",
       "291                     1.0  \n",
       "292                     1.0  \n",
       "\n",
       "[293 rows x 479 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694754fc",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "087737ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "result.to_csv(f'data/train/kickstarter_train_final_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)\n",
    "result.to_csv(f'data/test/kickstarter_test_final_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a41d3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there could be more than one categories for each project, create new features to split the categories.\n",
    "# df['new_category'] = df.category.str.split(\"/\", expand=False)\n",
    "# split_cat = pd.DataFrame(df['new_category'].tolist(), columns=['category1', 'category2'])\n",
    "# #each project should at least have 1 category, 'category2' can be \"None\". \n",
    "# #'category1' being the main category for the project.\n",
    "# df = pd.concat([df, split_cat], axis=1)\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4222",
   "language": "python",
   "name": "bt4222"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b14d2bd7895077ad303f266db7ad1f8a11e285bbfcdfa868008aad211f623e81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
