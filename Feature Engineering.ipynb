{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfcff7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "import pyLDAvis.gensim_models\n",
    "import en_core_web_md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b667a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Kickstarter_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a6a68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1404\n",
       "0     728\n",
       "Name: has_video, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'video' to a binary categorical variable\n",
    "df['video'].value_counts()\n",
    "df['has_video'] = df['video'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "df['has_video'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6f8c5",
   "metadata": {},
   "source": [
    "NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1445895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning for: rewards, description, description story, description risks\n",
    "\n",
    "def clean_text(df):\n",
    "    def process_rewards(corpus):\n",
    "    \n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row_processed = \"\"\n",
    "            row = row.replace(\"\\\\n\", \" \")\n",
    "            row = ast.literal_eval(row)\n",
    "\n",
    "            for dict in row:\n",
    "                row_processed += dict['rewards'].lower() + ' '\n",
    "            \n",
    "            \n",
    "            row_processed = row_processed.replace(\"//\",'')\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed)\n",
    "\n",
    "        return corpus_processed\n",
    "    \n",
    "    def process_description_story(corpus):\n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row = str(row)\n",
    "            row_processed = row.replace(\"\\r\", \" \" )\n",
    "            row_processed = row_processed.replace(\"\\n\", \" \" )\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed if not pd.isnull(row_processed) else \"\") # handle NA\n",
    "\n",
    "        return corpus_processed\n",
    "\n",
    "    df[\"rewards_processed\"] = process_rewards(df[\"rewards\"])\n",
    "    df[\"description_processed\"] = process_description_story(df[\"description\"])\n",
    "    df[\"description_story_processed\"] = process_description_story(df[\"description_story\"])\n",
    "    df[\"description_risks_processed\"] = process_description_story(df[\"description_risks\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95c3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizeTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c9ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nlp_features(df):\n",
    "    \n",
    "    # Rewards\n",
    "\n",
    "    vect_rewards = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    rewards_processed = pd.Series(df[\"rewards_processed\"])\n",
    "    tfidf_fit_rewards = vect_rewards.fit(rewards_processed)\n",
    "    rewards_tfidf_array = tfidf_fit_rewards.transform(rewards_processed).toarray()\n",
    "    rewards_tfidf_df = pd.DataFrame(rewards_tfidf_array)\n",
    "    rewards_tfidf_df.columns = list(map(lambda x : \"rewards_\" + str(x), rewards_tfidf_df.columns))\n",
    "    df = pd.merge(df, rewards_tfidf_df , left_index=True, right_index=True)\n",
    "    \n",
    "\n",
    "    # Description\n",
    "\n",
    "    vect_description = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    description = pd.Series(df[\"description_processed\"])\n",
    "    tfidf_fit_description = vect_description.fit(description)\n",
    "    description_tfidf_array = tfidf_fit_description.transform(description).toarray()\n",
    "    description_tfidf_df = pd.DataFrame(description_tfidf_array)\n",
    "    description_tfidf_df.columns = list(map(lambda x : \"description_\" + str(x), description_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Description Story\n",
    "\n",
    "    vect_description_story = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "    \n",
    "    description_story_processed = pd.Series(df[\"description_story_processed\"])\n",
    "    tfidf_fit_description_story_processed = vect_description_story.fit(description_story_processed)\n",
    "    description_story_processed_tfidf_array = tfidf_fit_description_story_processed.transform(description_story_processed).toarray()\n",
    "    description_story_tfidf_df = pd.DataFrame(description_story_processed_tfidf_array)\n",
    "    description_story_tfidf_df.columns = list(map(lambda x : \"description_story_\" + str(x), description_story_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_story_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Description Risks\n",
    "\n",
    "    vect_description_risks = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    description_risks_processed = pd.Series(df[\"description_risks_processed\"])\n",
    "    tfidf_fit_description_risks_processed = vect_description_risks.fit(description_risks_processed)\n",
    "    description_risks_processed_tfidf_array = tfidf_fit_description_risks_processed.transform(description_risks_processed).toarray()\n",
    "    description_risks_tfidf_df = pd.DataFrame(description_risks_processed_tfidf_array)\n",
    "    description_risks_tfidf_df.columns = list(map(lambda x : \"description_risks_\" + str(x), description_risks_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_risks_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "965ebbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bandy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\bandy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bandy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\bandy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bandy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\bandy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bandy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\bandy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "df = generate_nlp_features(clean_text(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dae2d5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "name\n",
      "description\n",
      "description_story\n",
      "description_risks\n",
      "rewards\n",
      "category\n",
      "pledged\n",
      "goal\n",
      "deadline\n",
      "location\n",
      "state\n",
      "faq_count\n",
      "update_count\n",
      "backers_count\n",
      "is_starrable\n",
      "spotlight\n",
      "staff_pick\n",
      "video\n",
      "creator_name\n",
      "creator_url\n",
      "url\n",
      "created_at\n",
      "published_at\n",
      "launched_at\n",
      "link\n",
      "has_video\n",
      "rewards_processed\n",
      "description_processed\n",
      "description_story_processed\n",
      "description_risks_processed\n",
      "rewards_0\n",
      "rewards_1\n",
      "rewards_2\n",
      "rewards_3\n",
      "rewards_4\n",
      "rewards_5\n",
      "rewards_6\n",
      "rewards_7\n",
      "rewards_8\n",
      "rewards_9\n",
      "rewards_10\n",
      "rewards_11\n",
      "rewards_12\n",
      "rewards_13\n",
      "rewards_14\n",
      "rewards_15\n",
      "rewards_16\n",
      "rewards_17\n",
      "rewards_18\n",
      "rewards_19\n",
      "rewards_20\n",
      "rewards_21\n",
      "rewards_22\n",
      "rewards_23\n",
      "rewards_24\n",
      "rewards_25\n",
      "rewards_26\n",
      "rewards_27\n",
      "rewards_28\n",
      "rewards_29\n",
      "rewards_30\n",
      "rewards_31\n",
      "rewards_32\n",
      "rewards_33\n",
      "rewards_34\n",
      "rewards_35\n",
      "rewards_36\n",
      "rewards_37\n",
      "rewards_38\n",
      "rewards_39\n",
      "rewards_40\n",
      "rewards_41\n",
      "rewards_42\n",
      "rewards_43\n",
      "rewards_44\n",
      "rewards_45\n",
      "rewards_46\n",
      "rewards_47\n",
      "rewards_48\n",
      "rewards_49\n",
      "rewards_50\n",
      "rewards_51\n",
      "rewards_52\n",
      "rewards_53\n",
      "rewards_54\n",
      "rewards_55\n",
      "rewards_56\n",
      "rewards_57\n",
      "rewards_58\n",
      "rewards_59\n",
      "rewards_60\n",
      "rewards_61\n",
      "rewards_62\n",
      "rewards_63\n",
      "rewards_64\n",
      "rewards_65\n",
      "rewards_66\n",
      "rewards_67\n",
      "rewards_68\n",
      "rewards_69\n",
      "rewards_70\n",
      "rewards_71\n",
      "rewards_72\n",
      "rewards_73\n",
      "rewards_74\n",
      "rewards_75\n",
      "rewards_76\n",
      "rewards_77\n",
      "rewards_78\n",
      "rewards_79\n",
      "rewards_80\n",
      "rewards_81\n",
      "rewards_82\n",
      "rewards_83\n",
      "rewards_84\n",
      "rewards_85\n",
      "rewards_86\n",
      "rewards_87\n",
      "rewards_88\n",
      "rewards_89\n",
      "rewards_90\n",
      "rewards_91\n",
      "rewards_92\n",
      "rewards_93\n",
      "rewards_94\n",
      "rewards_95\n",
      "rewards_96\n",
      "rewards_97\n",
      "rewards_98\n",
      "rewards_99\n",
      "description_0\n",
      "description_1\n",
      "description_2\n",
      "description_3\n",
      "description_4\n",
      "description_5\n",
      "description_6\n",
      "description_7\n",
      "description_8\n",
      "description_9\n",
      "description_10\n",
      "description_11\n",
      "description_12\n",
      "description_13\n",
      "description_14\n",
      "description_15\n",
      "description_16\n",
      "description_17\n",
      "description_18\n",
      "description_19\n",
      "description_20\n",
      "description_21\n",
      "description_22\n",
      "description_23\n",
      "description_24\n",
      "description_25\n",
      "description_26\n",
      "description_27\n",
      "description_28\n",
      "description_29\n",
      "description_30\n",
      "description_31\n",
      "description_32\n",
      "description_33\n",
      "description_34\n",
      "description_35\n",
      "description_36\n",
      "description_37\n",
      "description_38\n",
      "description_39\n",
      "description_40\n",
      "description_41\n",
      "description_42\n",
      "description_43\n",
      "description_44\n",
      "description_45\n",
      "description_46\n",
      "description_47\n",
      "description_48\n",
      "description_49\n",
      "description_50\n",
      "description_51\n",
      "description_52\n",
      "description_53\n",
      "description_54\n",
      "description_55\n",
      "description_56\n",
      "description_57\n",
      "description_58\n",
      "description_59\n",
      "description_60\n",
      "description_61\n",
      "description_62\n",
      "description_63\n",
      "description_64\n",
      "description_65\n",
      "description_66\n",
      "description_67\n",
      "description_68\n",
      "description_69\n",
      "description_70\n",
      "description_71\n",
      "description_72\n",
      "description_73\n",
      "description_74\n",
      "description_75\n",
      "description_76\n",
      "description_77\n",
      "description_78\n",
      "description_79\n",
      "description_80\n",
      "description_81\n",
      "description_82\n",
      "description_83\n",
      "description_84\n",
      "description_85\n",
      "description_86\n",
      "description_87\n",
      "description_88\n",
      "description_89\n",
      "description_90\n",
      "description_91\n",
      "description_92\n",
      "description_93\n",
      "description_94\n",
      "description_95\n",
      "description_96\n",
      "description_97\n",
      "description_98\n",
      "description_99\n",
      "description_story_0\n",
      "description_story_1\n",
      "description_story_2\n",
      "description_story_3\n",
      "description_story_4\n",
      "description_story_5\n",
      "description_story_6\n",
      "description_story_7\n",
      "description_story_8\n",
      "description_story_9\n",
      "description_story_10\n",
      "description_story_11\n",
      "description_story_12\n",
      "description_story_13\n",
      "description_story_14\n",
      "description_story_15\n",
      "description_story_16\n",
      "description_story_17\n",
      "description_story_18\n",
      "description_story_19\n",
      "description_story_20\n",
      "description_story_21\n",
      "description_story_22\n",
      "description_story_23\n",
      "description_story_24\n",
      "description_story_25\n",
      "description_story_26\n",
      "description_story_27\n",
      "description_story_28\n",
      "description_story_29\n",
      "description_story_30\n",
      "description_story_31\n",
      "description_story_32\n",
      "description_story_33\n",
      "description_story_34\n",
      "description_story_35\n",
      "description_story_36\n",
      "description_story_37\n",
      "description_story_38\n",
      "description_story_39\n",
      "description_story_40\n",
      "description_story_41\n",
      "description_story_42\n",
      "description_story_43\n",
      "description_story_44\n",
      "description_story_45\n",
      "description_story_46\n",
      "description_story_47\n",
      "description_story_48\n",
      "description_story_49\n",
      "description_story_50\n",
      "description_story_51\n",
      "description_story_52\n",
      "description_story_53\n",
      "description_story_54\n",
      "description_story_55\n",
      "description_story_56\n",
      "description_story_57\n",
      "description_story_58\n",
      "description_story_59\n",
      "description_story_60\n",
      "description_story_61\n",
      "description_story_62\n",
      "description_story_63\n",
      "description_story_64\n",
      "description_story_65\n",
      "description_story_66\n",
      "description_story_67\n",
      "description_story_68\n",
      "description_story_69\n",
      "description_story_70\n",
      "description_story_71\n",
      "description_story_72\n",
      "description_story_73\n",
      "description_story_74\n",
      "description_story_75\n",
      "description_story_76\n",
      "description_story_77\n",
      "description_story_78\n",
      "description_story_79\n",
      "description_story_80\n",
      "description_story_81\n",
      "description_story_82\n",
      "description_story_83\n",
      "description_story_84\n",
      "description_story_85\n",
      "description_story_86\n",
      "description_story_87\n",
      "description_story_88\n",
      "description_story_89\n",
      "description_story_90\n",
      "description_story_91\n",
      "description_story_92\n",
      "description_story_93\n",
      "description_story_94\n",
      "description_story_95\n",
      "description_story_96\n",
      "description_story_97\n",
      "description_story_98\n",
      "description_story_99\n",
      "description_risks_0\n",
      "description_risks_1\n",
      "description_risks_2\n",
      "description_risks_3\n",
      "description_risks_4\n",
      "description_risks_5\n",
      "description_risks_6\n",
      "description_risks_7\n",
      "description_risks_8\n",
      "description_risks_9\n",
      "description_risks_10\n",
      "description_risks_11\n",
      "description_risks_12\n",
      "description_risks_13\n",
      "description_risks_14\n",
      "description_risks_15\n",
      "description_risks_16\n",
      "description_risks_17\n",
      "description_risks_18\n",
      "description_risks_19\n",
      "description_risks_20\n",
      "description_risks_21\n",
      "description_risks_22\n",
      "description_risks_23\n",
      "description_risks_24\n",
      "description_risks_25\n",
      "description_risks_26\n",
      "description_risks_27\n",
      "description_risks_28\n",
      "description_risks_29\n",
      "description_risks_30\n",
      "description_risks_31\n",
      "description_risks_32\n",
      "description_risks_33\n",
      "description_risks_34\n",
      "description_risks_35\n",
      "description_risks_36\n",
      "description_risks_37\n",
      "description_risks_38\n",
      "description_risks_39\n",
      "description_risks_40\n",
      "description_risks_41\n",
      "description_risks_42\n",
      "description_risks_43\n",
      "description_risks_44\n",
      "description_risks_45\n",
      "description_risks_46\n",
      "description_risks_47\n",
      "description_risks_48\n",
      "description_risks_49\n",
      "description_risks_50\n",
      "description_risks_51\n",
      "description_risks_52\n",
      "description_risks_53\n",
      "description_risks_54\n",
      "description_risks_55\n",
      "description_risks_56\n",
      "description_risks_57\n",
      "description_risks_58\n",
      "description_risks_59\n",
      "description_risks_60\n",
      "description_risks_61\n",
      "description_risks_62\n",
      "description_risks_63\n",
      "description_risks_64\n",
      "description_risks_65\n",
      "description_risks_66\n",
      "description_risks_67\n",
      "description_risks_68\n",
      "description_risks_69\n",
      "description_risks_70\n",
      "description_risks_71\n",
      "description_risks_72\n",
      "description_risks_73\n",
      "description_risks_74\n",
      "description_risks_75\n",
      "description_risks_76\n",
      "description_risks_77\n",
      "description_risks_78\n",
      "description_risks_79\n",
      "description_risks_80\n",
      "description_risks_81\n",
      "description_risks_82\n",
      "description_risks_83\n",
      "description_risks_84\n",
      "description_risks_85\n",
      "description_risks_86\n",
      "description_risks_87\n",
      "description_risks_88\n",
      "description_risks_89\n",
      "description_risks_90\n",
      "description_risks_91\n",
      "description_risks_92\n",
      "description_risks_93\n",
      "description_risks_94\n",
      "description_risks_95\n",
      "description_risks_96\n",
      "description_risks_97\n",
      "description_risks_98\n",
      "description_risks_99\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883c447",
   "metadata": {},
   "source": [
    "**Rewards Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5880f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rewards_tiers(df):\n",
    "    df[\"reward_tiers\"] = df[\"rewards\"].apply(lambda x : len(ast.literal_eval(x)))\n",
    "    df = move_reward_tiers(df)\n",
    "    return df\n",
    "\n",
    "def create_all_reward_amount(df):\n",
    "    df[\"all_reward_amount\"] = np.empty((len(df), 0)).tolist()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        all_reward_amount = []\n",
    "        dict_list = ast.literal_eval(df.iloc[i, 5]) # Converts rewards column into dictionary\n",
    "\n",
    "        for dict in dict_list:\n",
    "            values_string = str(dict.values())\n",
    "            reward_title = re.search(r\"Pledge S\\$ \\d{1,3}(,\\d{1,3})? or more\", values_string) # Search for all reward titles\n",
    "\n",
    "            if reward_title is not None:\n",
    "                reward_amount = re.search(r\"\\d{1,3}(,\\d{1,3})?\", reward_title.group()) # Search for only the digits in reward amount\n",
    "                if reward_amount is not None:\n",
    "                    all_reward_amount.append(reward_amount.group())\n",
    "            else:\n",
    "                all_reward_amount.append(0) # If no reward title is found, add 0\n",
    "        df[\"all_reward_amount\"][i] = all_reward_amount\n",
    "    df = move_all_reward_amount(df)\n",
    "    return df\n",
    "\n",
    "# Rearange reward_tiers column to the right of rewards\n",
    "def move_reward_tiers(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[:6] + [cols[-1]] + cols[6:-1]\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "# Rearange all_reward_amount column to the right of reward_tiers\n",
    "def move_all_reward_amount(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[:7] + [cols[-1]] + cols[7:-1]\n",
    "    df = df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a640e9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_story</th>\n",
       "      <th>description_risks</th>\n",
       "      <th>rewards</th>\n",
       "      <th>reward_tiers</th>\n",
       "      <th>category</th>\n",
       "      <th>pledged</th>\n",
       "      <th>goal</th>\n",
       "      <th>...</th>\n",
       "      <th>description_risks_90</th>\n",
       "      <th>description_risks_91</th>\n",
       "      <th>description_risks_92</th>\n",
       "      <th>description_risks_93</th>\n",
       "      <th>description_risks_94</th>\n",
       "      <th>description_risks_95</th>\n",
       "      <th>description_risks_96</th>\n",
       "      <th>description_risks_97</th>\n",
       "      <th>description_risks_98</th>\n",
       "      <th>description_risks_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1249154571</td>\n",
       "      <td>Bunny Care Clinic Pin and Apparel Collection</td>\n",
       "      <td>A small collection of Bunny themed enamel pins...</td>\n",
       "      <td>Story\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\...</td>\n",
       "      <td>We try our best to keep everything on schedule...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>12</td>\n",
       "      <td>art/illustration</td>\n",
       "      <td>14115.0</td>\n",
       "      <td>700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1276054891</td>\n",
       "      <td>Hustle: A Singaporean Card Game</td>\n",
       "      <td>Hustle: A Singaporean Card Game is a funny and...</td>\n",
       "      <td>Story\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\...</td>\n",
       "      <td>We want to be frank and honest with all our ge...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>2</td>\n",
       "      <td>games/tabletop games</td>\n",
       "      <td>6.0</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140887</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>236207086</td>\n",
       "      <td>Neovide, Waterless One-Stop Sous Vide Cooker</td>\n",
       "      <td>No more water containers and vacuum bags. With...</td>\n",
       "      <td>Story\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\...</td>\n",
       "      <td>With our years of experience with products, ou...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>8</td>\n",
       "      <td>technology</td>\n",
       "      <td>289082.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2128144913</td>\n",
       "      <td>Lit Cafe</td>\n",
       "      <td>Little Toasts of Happiness</td>\n",
       "      <td>StoryHi! \\r\\nLit Cafe is a small space to prov...</td>\n",
       "      <td>The concept is to offer affordable local food ...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>7</td>\n",
       "      <td>food/spaces</td>\n",
       "      <td>170.0</td>\n",
       "      <td>12000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>891970407</td>\n",
       "      <td>Runway Tarot &amp; Golden Journey Tarot</td>\n",
       "      <td>When the fashion week come into Tarot.\\r\\nThis...</td>\n",
       "      <td>Story\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\...</td>\n",
       "      <td>COLLABORATION\\r\\nThis is the third time that b...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>15</td>\n",
       "      <td>art/painting</td>\n",
       "      <td>33599.0</td>\n",
       "      <td>6800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352060</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                          name  \\\n",
       "0  1249154571  Bunny Care Clinic Pin and Apparel Collection   \n",
       "1  1276054891               Hustle: A Singaporean Card Game   \n",
       "2   236207086  Neovide, Waterless One-Stop Sous Vide Cooker   \n",
       "3  2128144913                                      Lit Cafe   \n",
       "4   891970407           Runway Tarot & Golden Journey Tarot   \n",
       "\n",
       "                                         description  \\\n",
       "0  A small collection of Bunny themed enamel pins...   \n",
       "1  Hustle: A Singaporean Card Game is a funny and...   \n",
       "2  No more water containers and vacuum bags. With...   \n",
       "3                         Little Toasts of Happiness   \n",
       "4  When the fashion week come into Tarot.\\r\\nThis...   \n",
       "\n",
       "                                   description_story  \\\n",
       "0  Story\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\...   \n",
       "1  Story\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\...   \n",
       "2  Story\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\...   \n",
       "3  StoryHi! \\r\\nLit Cafe is a small space to prov...   \n",
       "4  Story\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\...   \n",
       "\n",
       "                                   description_risks  \\\n",
       "0  We try our best to keep everything on schedule...   \n",
       "1  We want to be frank and honest with all our ge...   \n",
       "2  With our years of experience with products, ou...   \n",
       "3  The concept is to offer affordable local food ...   \n",
       "4  COLLABORATION\\r\\nThis is the third time that b...   \n",
       "\n",
       "                                             rewards  reward_tiers  \\\n",
       "0  [{'rewards': 'Pledge without a reward'}, {'rew...            12   \n",
       "1  [{'rewards': 'Pledge without a reward'}, {'rew...             2   \n",
       "2  [{'rewards': 'Pledge without a reward'}, {'rew...             8   \n",
       "3  [{'rewards': 'Pledge without a reward'}, {'rew...             7   \n",
       "4  [{'rewards': 'Pledge without a reward'}, {'rew...            15   \n",
       "\n",
       "               category   pledged   goal  ... description_risks_90  \\\n",
       "0      art/illustration   14115.0    700  ...             0.000000   \n",
       "1  games/tabletop games       6.0     50  ...             0.000000   \n",
       "2            technology  289082.0  10000  ...             0.140606   \n",
       "3           food/spaces     170.0  12000  ...             0.000000   \n",
       "4          art/painting   33599.0   6800  ...             0.000000   \n",
       "\n",
       "  description_risks_91 description_risks_92 description_risks_93  \\\n",
       "0             0.296369             0.000000                  0.0   \n",
       "1             0.000000             0.000000                  0.0   \n",
       "2             0.000000             0.142717                  0.0   \n",
       "3             0.000000             0.000000                  0.0   \n",
       "4             0.000000             0.000000                  0.0   \n",
       "\n",
       "   description_risks_94  description_risks_95  description_risks_96  \\\n",
       "0              0.000000                   0.0                   0.0   \n",
       "1              0.000000                   0.0                   0.0   \n",
       "2              0.137243                   0.0                   0.0   \n",
       "3              0.000000                   0.0                   0.0   \n",
       "4              0.000000                   0.0                   0.0   \n",
       "\n",
       "   description_risks_97  description_risks_98 description_risks_99  \n",
       "0                   0.0              0.000000             0.000000  \n",
       "1                   0.0              0.140887             0.000000  \n",
       "2                   0.0              0.000000             0.133257  \n",
       "3                   0.0              0.000000             0.000000  \n",
       "4                   0.0              0.352060             0.000000  \n",
       "\n",
       "[5 rows x 432 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create reward tiers feature\n",
    "df = create_rewards_tiers(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e42c36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_story</th>\n",
       "      <th>description_risks</th>\n",
       "      <th>rewards</th>\n",
       "      <th>category</th>\n",
       "      <th>all_reward_amount</th>\n",
       "      <th>pledged</th>\n",
       "      <th>goal</th>\n",
       "      <th>...</th>\n",
       "      <th>staff_pick</th>\n",
       "      <th>video</th>\n",
       "      <th>creator_name</th>\n",
       "      <th>creator_url</th>\n",
       "      <th>url</th>\n",
       "      <th>created_at</th>\n",
       "      <th>published_at</th>\n",
       "      <th>launched_at</th>\n",
       "      <th>link</th>\n",
       "      <th>has_video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1249154571</td>\n",
       "      <td>Bunny Care Clinic Pin and Apparel Collection</td>\n",
       "      <td>A small collection of Bunny themed enamel pins...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHello! My...</td>\n",
       "      <td>We try our best to keep everything on schedule...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>art/illustration</td>\n",
       "      <td>[0, 12, 14, 20, 25, 32, 35, 78, 80, 85, 155, 130]</td>\n",
       "      <td>14115.0</td>\n",
       "      <td>700</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Labutori</td>\n",
       "      <td>https://www.kickstarter.com/profile/labutori</td>\n",
       "      <td>https://www.kickstarter.com/projects/labutori/...</td>\n",
       "      <td>2022-05-25 03:28:55+00:00</td>\n",
       "      <td>2022-09-09 01:25:20+00:00</td>\n",
       "      <td>2022-09-09 01:25:20+00:00</td>\n",
       "      <td>https://www.kickstarter.com/projects/labutori/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1276054891</td>\n",
       "      <td>Hustle: A Singaporean Card Game</td>\n",
       "      <td>Hustle: A Singaporean Card Game is a funny and...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>We want to be frank and honest with all our ge...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>games/tabletop games</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://v2.kickstarter.com/1662721387-XBw1i2Sj...</td>\n",
       "      <td>Hustle Singapore</td>\n",
       "      <td>https://www.kickstarter.com/profile/hustlesg</td>\n",
       "      <td>https://www.kickstarter.com/projects/hustlesg/...</td>\n",
       "      <td>2022-08-20 09:52:01+00:00</td>\n",
       "      <td>2022-09-07 13:08:05+00:00</td>\n",
       "      <td>2022-09-07 13:08:05+00:00</td>\n",
       "      <td>https://www.kickstarter.com/projects/hustlesg/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>236207086</td>\n",
       "      <td>Neovide, Waterless One-Stop Sous Vide Cooker</td>\n",
       "      <td>No more water containers and vacuum bags. With...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSous vide...</td>\n",
       "      <td>With our years of experience with products, ou...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>technology</td>\n",
       "      <td>[0, 2, 407, 505, 814, 293, 350, 463]</td>\n",
       "      <td>289082.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://v2.kickstarter.com/1662723490-gvkAMr9s...</td>\n",
       "      <td>The Space Tech</td>\n",
       "      <td>https://www.kickstarter.com/profile/thespacetech</td>\n",
       "      <td>https://www.kickstarter.com/projects/thespacet...</td>\n",
       "      <td>2022-06-30 09:28:52+00:00</td>\n",
       "      <td>2022-09-06 13:00:04+00:00</td>\n",
       "      <td>2022-09-06 13:00:04+00:00</td>\n",
       "      <td>https://www.kickstarter.com/projects/thespacet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2128144913</td>\n",
       "      <td>Lit Cafe</td>\n",
       "      <td>Little Toasts of Happiness</td>\n",
       "      <td>StoryHi! \\nLit Cafe is a small space to provid...</td>\n",
       "      <td>The concept is to offer affordable local food ...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>food/spaces</td>\n",
       "      <td>[0, 7, 12, 30, 38, 108, 168]</td>\n",
       "      <td>170.0</td>\n",
       "      <td>12000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kay En</td>\n",
       "      <td>https://www.kickstarter.com/profile/hosum</td>\n",
       "      <td>https://www.kickstarter.com/projects/hosum/ho-...</td>\n",
       "      <td>2022-08-30 08:28:52+00:00</td>\n",
       "      <td>2022-09-06 04:29:02+00:00</td>\n",
       "      <td>2022-09-06 04:29:02+00:00</td>\n",
       "      <td>https://www.kickstarter.com/projects/hosum/ho-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>891970407</td>\n",
       "      <td>Runway Tarot &amp; Golden Journey Tarot</td>\n",
       "      <td>When the fashion week come into Tarot.\\nThis p...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>COLLABORATION\\nThis is the third time that bot...</td>\n",
       "      <td>[{'rewards': 'Pledge without a reward'}, {'rew...</td>\n",
       "      <td>art/painting</td>\n",
       "      <td>[0, 2, 80, 85, 90, 94, 99, 150, 160, 165, 165,...</td>\n",
       "      <td>33599.0</td>\n",
       "      <td>6800</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://v2.kickstarter.com/1662720057-aeWt13h6...</td>\n",
       "      <td>Eugene Leong</td>\n",
       "      <td>https://www.kickstarter.com/profile/locationtarot</td>\n",
       "      <td>https://www.kickstarter.com/projects/locationt...</td>\n",
       "      <td>2022-07-18 14:48:29+00:00</td>\n",
       "      <td>2022-09-05 13:57:35+00:00</td>\n",
       "      <td>2022-09-05 13:57:35+00:00</td>\n",
       "      <td>https://www.kickstarter.com/projects/locationt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                          name  \\\n",
       "0  1249154571  Bunny Care Clinic Pin and Apparel Collection   \n",
       "1  1276054891               Hustle: A Singaporean Card Game   \n",
       "2   236207086  Neovide, Waterless One-Stop Sous Vide Cooker   \n",
       "3  2128144913                                      Lit Cafe   \n",
       "4   891970407           Runway Tarot & Golden Journey Tarot   \n",
       "\n",
       "                                         description  \\\n",
       "0  A small collection of Bunny themed enamel pins...   \n",
       "1  Hustle: A Singaporean Card Game is a funny and...   \n",
       "2  No more water containers and vacuum bags. With...   \n",
       "3                         Little Toasts of Happiness   \n",
       "4  When the fashion week come into Tarot.\\nThis p...   \n",
       "\n",
       "                                   description_story  \\\n",
       "0  Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHello! My...   \n",
       "1  Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "2  Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSous vide...   \n",
       "3  StoryHi! \\nLit Cafe is a small space to provid...   \n",
       "4  Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                   description_risks  \\\n",
       "0  We try our best to keep everything on schedule...   \n",
       "1  We want to be frank and honest with all our ge...   \n",
       "2  With our years of experience with products, ou...   \n",
       "3  The concept is to offer affordable local food ...   \n",
       "4  COLLABORATION\\nThis is the third time that bot...   \n",
       "\n",
       "                                             rewards              category  \\\n",
       "0  [{'rewards': 'Pledge without a reward'}, {'rew...      art/illustration   \n",
       "1  [{'rewards': 'Pledge without a reward'}, {'rew...  games/tabletop games   \n",
       "2  [{'rewards': 'Pledge without a reward'}, {'rew...            technology   \n",
       "3  [{'rewards': 'Pledge without a reward'}, {'rew...           food/spaces   \n",
       "4  [{'rewards': 'Pledge without a reward'}, {'rew...          art/painting   \n",
       "\n",
       "                                   all_reward_amount   pledged   goal  ...  \\\n",
       "0  [0, 12, 14, 20, 25, 32, 35, 78, 80, 85, 155, 130]   14115.0    700  ...   \n",
       "1                                             [0, 2]       6.0     50  ...   \n",
       "2               [0, 2, 407, 505, 814, 293, 350, 463]  289082.0  10000  ...   \n",
       "3                       [0, 7, 12, 30, 38, 108, 168]     170.0  12000  ...   \n",
       "4  [0, 2, 80, 85, 90, 94, 99, 150, 160, 165, 165,...   33599.0   6800  ...   \n",
       "\n",
       "  staff_pick                                              video  \\\n",
       "0          0                                                NaN   \n",
       "1          0  https://v2.kickstarter.com/1662721387-XBw1i2Sj...   \n",
       "2          1  https://v2.kickstarter.com/1662723490-gvkAMr9s...   \n",
       "3          0                                                NaN   \n",
       "4          0  https://v2.kickstarter.com/1662720057-aeWt13h6...   \n",
       "\n",
       "       creator_name                                        creator_url  \\\n",
       "0          Labutori       https://www.kickstarter.com/profile/labutori   \n",
       "1  Hustle Singapore       https://www.kickstarter.com/profile/hustlesg   \n",
       "2    The Space Tech   https://www.kickstarter.com/profile/thespacetech   \n",
       "3            Kay En          https://www.kickstarter.com/profile/hosum   \n",
       "4      Eugene Leong  https://www.kickstarter.com/profile/locationtarot   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.kickstarter.com/projects/labutori/...   \n",
       "1  https://www.kickstarter.com/projects/hustlesg/...   \n",
       "2  https://www.kickstarter.com/projects/thespacet...   \n",
       "3  https://www.kickstarter.com/projects/hosum/ho-...   \n",
       "4  https://www.kickstarter.com/projects/locationt...   \n",
       "\n",
       "                  created_at               published_at  \\\n",
       "0  2022-05-25 03:28:55+00:00  2022-09-09 01:25:20+00:00   \n",
       "1  2022-08-20 09:52:01+00:00  2022-09-07 13:08:05+00:00   \n",
       "2  2022-06-30 09:28:52+00:00  2022-09-06 13:00:04+00:00   \n",
       "3  2022-08-30 08:28:52+00:00  2022-09-06 04:29:02+00:00   \n",
       "4  2022-07-18 14:48:29+00:00  2022-09-05 13:57:35+00:00   \n",
       "\n",
       "                 launched_at  \\\n",
       "0  2022-09-09 01:25:20+00:00   \n",
       "1  2022-09-07 13:08:05+00:00   \n",
       "2  2022-09-06 13:00:04+00:00   \n",
       "3  2022-09-06 04:29:02+00:00   \n",
       "4  2022-09-05 13:57:35+00:00   \n",
       "\n",
       "                                                link has_video  \n",
       "0  https://www.kickstarter.com/projects/labutori/...         0  \n",
       "1  https://www.kickstarter.com/projects/hustlesg/...         1  \n",
       "2  https://www.kickstarter.com/projects/thespacet...         1  \n",
       "3  https://www.kickstarter.com/projects/hosum/ho-...         0  \n",
       "4  https://www.kickstarter.com/projects/locationt...         1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create all reward amount feature\n",
    "# If lowest reward amount is not 0, the project is either already fully funded or cancelled.\n",
    "# Rewards should be sorted in ascending order, any amount to the right and less than the max means reward is no longer available.\n",
    "df = create_all_reward_amount(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].\n",
    "'''\n",
    "def generate_sentiment_features():\n",
    "    df = df.dropna(subset=['description_story_processed', 'description_risks_processed', 'description_proocessed', 'rewards_processed']) # NOTE: put at top with other dropnas from other features?\n",
    "    df[\"description_story_polarity\"] = df[\"description_story_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_story_subjectivity\"] = df[\"description_story_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"description_polarity\"] = df[\"description_proocessed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_subjectivity\"] = df[\"description_proocessed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"description_risks_polarity\"] = df[\"description_risks_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_risks_subjectivity\"] = df[\"description_risks_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"rewards_polarity\"] = df[\"rewards_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"rewards_subjectivity\"] = df[\"rewards_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8913eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate word_count_features(): # omitted description due to word limit, word count likely similar for all projects\n",
    "    df['description_story_word_count'] = df[\"description_story_processed\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['description_risks_word_count'] = df[\"description_risks_processed\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['rewards_word_count'] = df[\"rewards_processed\"].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf488355",
   "metadata": {},
   "source": [
    "hold out on topic modelling first because it is unsupervised algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "814d4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = clean_text(df)\n",
    "# #generate BOW on description_story \n",
    "# #Our spaCy model:\n",
    "# nlp = en_core_web_md.load()\n",
    "# # Tags I want to remove from the text\n",
    "# removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "# tokens = []\n",
    "\n",
    "# for story in nlp.pipe(df['description_story_processed']):\n",
    "#    proj_tok = [token.lemma_.lower() for token in story if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
    "#    tokens.append(proj_tok)\n",
    "    \n",
    "# dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "# # No_below: Tokens that appear in less than 5 documents are filtered out.\n",
    "# # No_above: Tokens that appear in more than 50% of the total corpus are also removed as default.\n",
    "# # Keep_n: We limit ourselves to the top 1000 most frequent tokens (default is 100.000). Set to ‘None’ if you want to keep all.\n",
    "\n",
    "# df['story_tokens'] = tokens\n",
    "# print(len(df))\n",
    "# print(len(tokens))\n",
    "\n",
    "# df['story_tokens']\n",
    "# dictionary = Dictionary(df['story_tokens'])\n",
    "\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in df['story_tokens']]\n",
    "\n",
    "# lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, num_topics=10, workers = 4, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe67541",
   "metadata": {},
   "source": [
    "## Combine all feature generating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clean_text(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4222",
   "language": "python",
   "name": "bt4222"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b14d2bd7895077ad303f266db7ad1f8a11e285bbfcdfa868008aad211f623e81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
