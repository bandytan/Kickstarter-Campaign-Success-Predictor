{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcff7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/gensim/matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import triu\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ivankoh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "import pyLDAvis.gensim_models\n",
    "import en_core_web_md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b667a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/kickstarter_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a6a68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1015\n",
       "0     542\n",
       "Name: has_video, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'video' to a binary categorical variable\n",
    "df['video'].value_counts()\n",
    "df['has_video'] = df['video'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "df['has_video'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6f8c5",
   "metadata": {},
   "source": [
    "NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1445895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning for: rewards, description, description story, description risks\n",
    "\n",
    "def clean_text(df):\n",
    "    def process_rewards(corpus):\n",
    "    \n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row_processed = \"\"\n",
    "            row = row.replace(\"\\\\n\", \" \")\n",
    "            row = ast.literal_eval(row)\n",
    "\n",
    "            for dict in row:\n",
    "                row_processed += dict['rewards'].lower() + ' '\n",
    "            \n",
    "            \n",
    "            row_processed = row_processed.replace(\"//\",'')\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed)\n",
    "\n",
    "        return corpus_processed\n",
    "    \n",
    "    def process_description_story(corpus):\n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row = str(row)\n",
    "            row_processed = row.replace(\"\\r\", \" \" )\n",
    "            row_processed = row_processed.replace(\"\\n\", \" \" )\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed if not pd.isnull(row_processed) else \"\") # handle NA\n",
    "\n",
    "        return corpus_processed\n",
    "\n",
    "    df[\"rewards_processed\"] = process_rewards(df[\"rewards\"])\n",
    "    df[\"description_processed\"] = process_description_story(df[\"description\"])\n",
    "    df[\"description_story_processed\"] = process_description_story(df[\"description_story\"])\n",
    "    df[\"description_risks_processed\"] = process_description_story(df[\"description_risks\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95c3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizeTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c9ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nlp_features(df):\n",
    "    \n",
    "    # Rewards\n",
    "\n",
    "    vect_rewards = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    rewards_processed = pd.Series(df[\"rewards_processed\"])\n",
    "    tfidf_fit_rewards = vect_rewards.fit(rewards_processed)\n",
    "    rewards_tfidf_array = tfidf_fit_rewards.transform(rewards_processed).toarray()\n",
    "    rewards_tfidf_df = pd.DataFrame(rewards_tfidf_array)\n",
    "    rewards_tfidf_df.columns = list(map(lambda x : \"rewards_\" + str(x), rewards_tfidf_df.columns))\n",
    "    df = pd.merge(df, rewards_tfidf_df , left_index=True, right_index=True)\n",
    "    \n",
    "\n",
    "    # Description\n",
    "\n",
    "    vect_description = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    description = pd.Series(df[\"description_processed\"])\n",
    "    tfidf_fit_description = vect_description.fit(description)\n",
    "    description_tfidf_array = tfidf_fit_description.transform(description).toarray()\n",
    "    description_tfidf_df = pd.DataFrame(description_tfidf_array)\n",
    "    description_tfidf_df.columns = list(map(lambda x : \"description_\" + str(x), description_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Description Story\n",
    "\n",
    "    vect_description_story = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "    \n",
    "    description_story_processed = pd.Series(df[\"description_story_processed\"])\n",
    "    tfidf_fit_description_story_processed = vect_description_story.fit(description_story_processed)\n",
    "    description_story_processed_tfidf_array = tfidf_fit_description_story_processed.transform(description_story_processed).toarray()\n",
    "    description_story_tfidf_df = pd.DataFrame(description_story_processed_tfidf_array)\n",
    "    description_story_tfidf_df.columns = list(map(lambda x : \"description_story_\" + str(x), description_story_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_story_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Description Risks\n",
    "\n",
    "    vect_description_risks = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    description_risks_processed = pd.Series(df[\"description_risks_processed\"])\n",
    "    tfidf_fit_description_risks_processed = vect_description_risks.fit(description_risks_processed)\n",
    "    description_risks_processed_tfidf_array = tfidf_fit_description_risks_processed.transform(description_risks_processed).toarray()\n",
    "    description_risks_tfidf_df = pd.DataFrame(description_risks_processed_tfidf_array)\n",
    "    description_risks_tfidf_df.columns = list(map(lambda x : \"description_risks_\" + str(x), description_risks_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_risks_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "965ebbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "#df = generate_nlp_features(clean_text(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae2d5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "name\n",
      "description\n",
      "description_story\n",
      "description_risks\n",
      "rewards\n",
      "category\n",
      "pledged\n",
      "goal\n",
      "deadline\n",
      "location\n",
      "state\n",
      "faq_count\n",
      "update_count\n",
      "backers_count\n",
      "spotlight\n",
      "staff_pick\n",
      "video\n",
      "launched_at\n",
      "has_video\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883c447",
   "metadata": {},
   "source": [
    "**Rewards Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5880f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rewards_tiers(df):\n",
    "    df[\"reward_tiers\"] = df[\"rewards\"].apply(lambda x : len(ast.literal_eval(x)))\n",
    "    df = move_reward_tiers(df)\n",
    "    return df\n",
    "\n",
    "def create_all_reward_amount(df):\n",
    "    df[\"all_reward_amount\"] = np.empty((len(df), 0)).tolist()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        all_reward_amount = []\n",
    "        dict_list = ast.literal_eval(df.iloc[i, 5]) # Converts rewards column into dictionary\n",
    "\n",
    "        for dict in dict_list:\n",
    "            values_string = str(dict.values())\n",
    "            reward_title = re.search(r\"Pledge S\\$ \\d{1,3}(,\\d{1,3})? or more\", values_string) # Search for all reward titles\n",
    "\n",
    "            if reward_title is not None:\n",
    "                reward_amount = re.search(r\"\\d{1,3}(,\\d{1,3})?\", reward_title.group()) # Search for only the digits in reward amount\n",
    "                if reward_amount is not None:\n",
    "                    all_reward_amount.append(reward_amount.group())\n",
    "            else:\n",
    "                all_reward_amount.append(0) # If no reward title is found, add 0\n",
    "        df[\"all_reward_amount\"][i] = all_reward_amount\n",
    "    df = move_all_reward_amount(df)\n",
    "    return df\n",
    "\n",
    "# Rearange reward_tiers column to the right of rewards\n",
    "def move_reward_tiers(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[:6] + [cols[-1]] + cols[6:-1]\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "# Rearange all_reward_amount column to the right of reward_tiers\n",
    "def move_all_reward_amount(df):\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[:7] + [cols[-1]] + cols[7:-1]\n",
    "    df = df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a640e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reward tiers feature\n",
    "# df = create_rewards_tiers(df)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e42c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all reward amount feature\n",
    "# If lowest reward amount is not 0, the project is either already fully funded or cancelled.\n",
    "# Rewards should be sorted in ascending order, any amount to the right and less than the max means reward is no longer available.\n",
    "# df = create_all_reward_amount(df)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2008d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].\n",
    "'''\n",
    "def generate_sentiment_features(df):\n",
    "    df = df.dropna(subset=['description_story_processed', 'description_risks_processed', 'description_processed', 'rewards_processed']) # NOTE: put at top with other dropnas from other features?\n",
    "    df[\"description_story_polarity\"] = df[\"description_story_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_story_subjectivity\"] = df[\"description_story_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"description_polarity\"] = df[\"description_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_subjectivity\"] = df[\"description_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"description_risks_polarity\"] = df[\"description_risks_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"description_risks_subjectivity\"] = df[\"description_risks_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)\n",
    "    df[\"rewards_polarity\"] = df[\"rewards_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.polarity)\n",
    "    df[\"rewards_subjectivity\"] = df[\"rewards_processed\"].apply(lambda x: \n",
    "                   TextBlob(x).sentiment.subjectivity)          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb8913eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_count_features(df): # omitted description due to word limit, word count likely similar for all projects\n",
    "    df['description_story_word_count'] = df[\"description_story_processed\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['description_risks_word_count'] = df[\"description_risks_processed\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "    df['rewards_word_count'] = df[\"rewards_processed\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e87cf",
   "metadata": {},
   "source": [
    "when to drop one of the OHE columns: \n",
    "https://stats.stackexchange.com/questions/231285/dropping-one-of-the-columns-when-using-one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d13b9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(col, df):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    enc.fit(df[col].unique().reshape(-1, 1))\n",
    "    transformed = enc.transform(df[col].to_numpy().reshape(-1, 1))\n",
    "    #Create a Pandas DataFrame of the hot encoded column\n",
    "    ohe_df = pd.DataFrame(transformed, columns=enc.get_feature_names_out())\n",
    "    #concat with original data\n",
    "    data = pd.concat([df, ohe_df], axis=1).drop([col], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf488355",
   "metadata": {},
   "source": [
    "hold out on topic modelling first because it is unsupervised algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "814d4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = clean_text(df)\n",
    "# #generate BOW on description_story \n",
    "# #Our spaCy model:\n",
    "# nlp = en_core_web_md.load()\n",
    "# # Tags I want to remove from the text\n",
    "# removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "# tokens = []\n",
    "\n",
    "# for story in nlp.pipe(df['description_story_processed']):\n",
    "#    proj_tok = [token.lemma_.lower() for token in story if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
    "#    tokens.append(proj_tok)\n",
    "    \n",
    "# dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "# # No_below: Tokens that appear in less than 5 documents are filtered out.\n",
    "# # No_above: Tokens that appear in more than 50% of the total corpus are also removed as default.\n",
    "# # Keep_n: We limit ourselves to the top 1000 most frequent tokens (default is 100.000). Set to ‘None’ if you want to keep all.\n",
    "\n",
    "# df['story_tokens'] = tokens\n",
    "# print(len(df))\n",
    "# print(len(tokens))\n",
    "\n",
    "# df['story_tokens']\n",
    "# dictionary = Dictionary(df['story_tokens'])\n",
    "\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in df['story_tokens']]\n",
    "\n",
    "# lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, num_topics=10, workers = 4, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe67541",
   "metadata": {},
   "source": [
    "## Combine all feature generating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35d0b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/ivankoh/opt/miniconda3/envs/bt4222/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_69727/2749083875.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"all_reward_amount\"][i] = all_reward_amount\n"
     ]
    }
   ],
   "source": [
    "result = clean_text(df)\n",
    "result = generate_nlp_features(result)\n",
    "result = create_rewards_tiers(result)\n",
    "result = create_all_reward_amount(result)\n",
    "result = generate_sentiment_features(result)\n",
    "result = generate_word_count_features(result)\n",
    "result = ohe('category', result)\n",
    "result = ohe('location', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bad7307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_story</th>\n",
       "      <th>description_risks</th>\n",
       "      <th>rewards</th>\n",
       "      <th>reward_tiers</th>\n",
       "      <th>all_reward_amount</th>\n",
       "      <th>pledged</th>\n",
       "      <th>goal</th>\n",
       "      <th>...</th>\n",
       "      <th>x0_technology/web</th>\n",
       "      <th>x0_theater/comedy</th>\n",
       "      <th>x0_theater/experimental</th>\n",
       "      <th>x0_theater/immersive</th>\n",
       "      <th>x0_theater/musical</th>\n",
       "      <th>x0_theater/plays</th>\n",
       "      <th>x0_Orchard, Singapore</th>\n",
       "      <th>x0_Queenstown, Singapore</th>\n",
       "      <th>x0_Sembawang, Singapore</th>\n",
       "      <th>x0_Singapore, Singapore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1691565384</td>\n",
       "      <td>Make 100: City maps weaved of cassette tapes</td>\n",
       "      <td>Personalised a map that contains curated music...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou'll need a...</td>\n",
       "      <td>We thank all of our cassette tape donors who c...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 90 or more\\n\\nAbout $6...</td>\n",
       "      <td>4</td>\n",
       "      <td>[90, 175, 249, 250]</td>\n",
       "      <td>2138.00</td>\n",
       "      <td>800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320949924</td>\n",
       "      <td>Retrograde Hard Enamel Pins</td>\n",
       "      <td>A series of Enamel Pins based on the combinati...</td>\n",
       "      <td>StoryA series of pins based on a set of illust...</td>\n",
       "      <td>- The possibility of minor changes in details ...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 10 or more\\n\\nAbout US...</td>\n",
       "      <td>9</td>\n",
       "      <td>[10, 17, 17, 34, 34, 50, 50, 64, 64]</td>\n",
       "      <td>10476.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956852023</td>\n",
       "      <td>Owl-Carina: The Sound of Wings | MAKE 100</td>\n",
       "      <td>Cute handmade owl ocarinas and whistles. Potte...</td>\n",
       "      <td>Story\\n\\n\\nOwl-carina\\n\\n\\n\\n\\nWhat can these ...</td>\n",
       "      <td>Shipping and delivery might break the pieces. ...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 2 or more\\n\\nAbout $2\\...</td>\n",
       "      <td>5</td>\n",
       "      <td>[2, 24, 39, 68, 129]</td>\n",
       "      <td>7316.00</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1565012664</td>\n",
       "      <td>Alohomora - Magical Automated Safety Gate Unlo...</td>\n",
       "      <td>Alohomora, an automated unlocking module for b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'rewards': \"Pledge S$ 2 or more\\n\\nAbout $2\\...</td>\n",
       "      <td>7</td>\n",
       "      <td>[2, 5, 10, 25, 39, 49, 89]</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>3000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1255089705</td>\n",
       "      <td>Vario WW1 1918 Trench Watch</td>\n",
       "      <td>Vario's 3rd watch collection inspired by WW1 p...</td>\n",
       "      <td>StoryAll watches will come with a strap and ba...</td>\n",
       "      <td>We've had numerous successful crowdfunding cam...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 20 or more\\n\\nAbout US...</td>\n",
       "      <td>25</td>\n",
       "      <td>[20, 32, 340, 340, 340, 340, 340, 340, 340, 34...</td>\n",
       "      <td>297149.03</td>\n",
       "      <td>30000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>730883734</td>\n",
       "      <td>Pokepuffs: RGB Starters Enamel Pins</td>\n",
       "      <td>A collection of Hard Enamel Pins featuring cut...</td>\n",
       "      <td>Story** ALL DESIGNS ARE UNLOCKED!**\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>There remains the possibility of delays in pro...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 12 or more\\n\\nAbout US...</td>\n",
       "      <td>17</td>\n",
       "      <td>[12, 24, 36, 48, 60, 72, 84, 96, 105, 138, 173...</td>\n",
       "      <td>10478.00</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>1520786796</td>\n",
       "      <td>HOROCD - The Perfect Cleaning Solution for all...</td>\n",
       "      <td>The horophile’s solution to clean and shiny wa...</td>\n",
       "      <td>Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWe are pr...</td>\n",
       "      <td>HOROCD has undergone intensive concoction and ...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 48 or more\\n\\nAbout $3...</td>\n",
       "      <td>9</td>\n",
       "      <td>[48, 58, 68, 128, 158, 198, 258, 960, 1,780]</td>\n",
       "      <td>3614.00</td>\n",
       "      <td>2000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>209727311</td>\n",
       "      <td>From where to wear, go afield with BOLDR Exped...</td>\n",
       "      <td>The BOLDR Expedition Swiss Automatic field wat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 539 or more\\n\\nAbout $...</td>\n",
       "      <td>14</td>\n",
       "      <td>[539, 539, 539, 539, 539, 579, 579, 579, 579, ...</td>\n",
       "      <td>229928.00</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>820148443</td>\n",
       "      <td>Kamen Rider Armor Sticker set</td>\n",
       "      <td>A sticker set of Kamen Rider Zio</td>\n",
       "      <td>StoryThis year we want to do at least one big ...</td>\n",
       "      <td>Prices might change. Singapore is going throug...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 10 or more\\n\\nAbout $8...</td>\n",
       "      <td>4</td>\n",
       "      <td>[10, 10, 15, 25]</td>\n",
       "      <td>151.00</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>262192401</td>\n",
       "      <td>3D Print Fashion Collection about Covid-19</td>\n",
       "      <td>An avant garde fashion collection inspired by ...</td>\n",
       "      <td>Storyhello everyone! this is my fashion projec...</td>\n",
       "      <td>There isnt much risk involve because I have th...</td>\n",
       "      <td>[{'rewards': 'Pledge S$ 30 or more\\n\\nAbout $2...</td>\n",
       "      <td>2</td>\n",
       "      <td>[30, 100]</td>\n",
       "      <td>62.00</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1557 rows × 560 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                               name  \\\n",
       "0     1691565384       Make 100: City maps weaved of cassette tapes   \n",
       "1      320949924                        Retrograde Hard Enamel Pins   \n",
       "2     1956852023          Owl-Carina: The Sound of Wings | MAKE 100   \n",
       "3     1565012664  Alohomora - Magical Automated Safety Gate Unlo...   \n",
       "4     1255089705                        Vario WW1 1918 Trench Watch   \n",
       "...          ...                                                ...   \n",
       "1552   730883734                Pokepuffs: RGB Starters Enamel Pins   \n",
       "1553  1520786796  HOROCD - The Perfect Cleaning Solution for all...   \n",
       "1554   209727311  From where to wear, go afield with BOLDR Exped...   \n",
       "1555   820148443                      Kamen Rider Armor Sticker set   \n",
       "1556   262192401         3D Print Fashion Collection about Covid-19   \n",
       "\n",
       "                                            description  \\\n",
       "0     Personalised a map that contains curated music...   \n",
       "1     A series of Enamel Pins based on the combinati...   \n",
       "2     Cute handmade owl ocarinas and whistles. Potte...   \n",
       "3     Alohomora, an automated unlocking module for b...   \n",
       "4     Vario's 3rd watch collection inspired by WW1 p...   \n",
       "...                                                 ...   \n",
       "1552  A collection of Hard Enamel Pins featuring cut...   \n",
       "1553  The horophile’s solution to clean and shiny wa...   \n",
       "1554  The BOLDR Expedition Swiss Automatic field wat...   \n",
       "1555                   A sticker set of Kamen Rider Zio   \n",
       "1556  An avant garde fashion collection inspired by ...   \n",
       "\n",
       "                                      description_story  \\\n",
       "0     Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou'll need a...   \n",
       "1     StoryA series of pins based on a set of illust...   \n",
       "2     Story\\n\\n\\nOwl-carina\\n\\n\\n\\n\\nWhat can these ...   \n",
       "3                                                   NaN   \n",
       "4     StoryAll watches will come with a strap and ba...   \n",
       "...                                                 ...   \n",
       "1552  Story** ALL DESIGNS ARE UNLOCKED!**\\n\\n\\n\\n\\n\\...   \n",
       "1553  Story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWe are pr...   \n",
       "1554                                                NaN   \n",
       "1555  StoryThis year we want to do at least one big ...   \n",
       "1556  Storyhello everyone! this is my fashion projec...   \n",
       "\n",
       "                                      description_risks  \\\n",
       "0     We thank all of our cassette tape donors who c...   \n",
       "1     - The possibility of minor changes in details ...   \n",
       "2     Shipping and delivery might break the pieces. ...   \n",
       "3                                                   NaN   \n",
       "4     We've had numerous successful crowdfunding cam...   \n",
       "...                                                 ...   \n",
       "1552  There remains the possibility of delays in pro...   \n",
       "1553  HOROCD has undergone intensive concoction and ...   \n",
       "1554                                                NaN   \n",
       "1555  Prices might change. Singapore is going throug...   \n",
       "1556  There isnt much risk involve because I have th...   \n",
       "\n",
       "                                                rewards  reward_tiers  \\\n",
       "0     [{'rewards': 'Pledge S$ 90 or more\\n\\nAbout $6...             4   \n",
       "1     [{'rewards': 'Pledge S$ 10 or more\\n\\nAbout US...             9   \n",
       "2     [{'rewards': 'Pledge S$ 2 or more\\n\\nAbout $2\\...             5   \n",
       "3     [{'rewards': \"Pledge S$ 2 or more\\n\\nAbout $2\\...             7   \n",
       "4     [{'rewards': 'Pledge S$ 20 or more\\n\\nAbout US...            25   \n",
       "...                                                 ...           ...   \n",
       "1552  [{'rewards': 'Pledge S$ 12 or more\\n\\nAbout US...            17   \n",
       "1553  [{'rewards': 'Pledge S$ 48 or more\\n\\nAbout $3...             9   \n",
       "1554  [{'rewards': 'Pledge S$ 539 or more\\n\\nAbout $...            14   \n",
       "1555  [{'rewards': 'Pledge S$ 10 or more\\n\\nAbout $8...             4   \n",
       "1556  [{'rewards': 'Pledge S$ 30 or more\\n\\nAbout $2...             2   \n",
       "\n",
       "                                      all_reward_amount    pledged   goal  \\\n",
       "0                                   [90, 175, 249, 250]    2138.00    800   \n",
       "1                  [10, 17, 17, 34, 34, 50, 50, 64, 64]   10476.00   1200   \n",
       "2                                  [2, 24, 39, 68, 129]    7316.00    100   \n",
       "3                            [2, 5, 10, 25, 39, 49, 89]    2437.00   3000   \n",
       "4     [20, 32, 340, 340, 340, 340, 340, 340, 340, 34...  297149.03  30000   \n",
       "...                                                 ...        ...    ...   \n",
       "1552  [12, 24, 36, 48, 60, 72, 84, 96, 105, 138, 173...   10478.00    300   \n",
       "1553       [48, 58, 68, 128, 158, 198, 258, 960, 1,780]    3614.00   2000   \n",
       "1554  [539, 539, 539, 539, 539, 579, 579, 579, 579, ...  229928.00  50000   \n",
       "1555                                   [10, 10, 15, 25]     151.00    100   \n",
       "1556                                          [30, 100]      62.00    500   \n",
       "\n",
       "      ... x0_technology/web x0_theater/comedy x0_theater/experimental  \\\n",
       "0     ...               0.0               0.0                     0.0   \n",
       "1     ...               0.0               0.0                     0.0   \n",
       "2     ...               0.0               0.0                     0.0   \n",
       "3     ...               0.0               0.0                     0.0   \n",
       "4     ...               0.0               0.0                     0.0   \n",
       "...   ...               ...               ...                     ...   \n",
       "1552  ...               0.0               0.0                     0.0   \n",
       "1553  ...               0.0               0.0                     0.0   \n",
       "1554  ...               0.0               0.0                     0.0   \n",
       "1555  ...               0.0               0.0                     0.0   \n",
       "1556  ...               0.0               0.0                     0.0   \n",
       "\n",
       "      x0_theater/immersive  x0_theater/musical  x0_theater/plays  \\\n",
       "0                      0.0                 0.0               0.0   \n",
       "1                      0.0                 0.0               0.0   \n",
       "2                      0.0                 0.0               0.0   \n",
       "3                      0.0                 0.0               0.0   \n",
       "4                      0.0                 0.0               0.0   \n",
       "...                    ...                 ...               ...   \n",
       "1552                   0.0                 0.0               0.0   \n",
       "1553                   0.0                 0.0               0.0   \n",
       "1554                   0.0                 0.0               0.0   \n",
       "1555                   0.0                 0.0               0.0   \n",
       "1556                   0.0                 0.0               0.0   \n",
       "\n",
       "      x0_Orchard, Singapore x0_Queenstown, Singapore x0_Sembawang, Singapore  \\\n",
       "0                       0.0                      0.0                     0.0   \n",
       "1                       0.0                      0.0                     0.0   \n",
       "2                       0.0                      0.0                     0.0   \n",
       "3                       0.0                      0.0                     0.0   \n",
       "4                       0.0                      0.0                     0.0   \n",
       "...                     ...                      ...                     ...   \n",
       "1552                    0.0                      0.0                     0.0   \n",
       "1553                    0.0                      0.0                     0.0   \n",
       "1554                    0.0                      0.0                     0.0   \n",
       "1555                    0.0                      0.0                     0.0   \n",
       "1556                    0.0                      0.0                     0.0   \n",
       "\n",
       "      x0_Singapore, Singapore  \n",
       "0                         1.0  \n",
       "1                         1.0  \n",
       "2                         1.0  \n",
       "3                         1.0  \n",
       "4                         1.0  \n",
       "...                       ...  \n",
       "1552                      1.0  \n",
       "1553                      1.0  \n",
       "1554                      1.0  \n",
       "1555                      1.0  \n",
       "1556                      1.0  \n",
       "\n",
       "[1557 rows x 560 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694754fc",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "087737ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "result.to_csv(f'data/kickstarter_train_final_{time.strftime(\"%Y%m%d-%H%M%S\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a41d3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there could be more than one categories for each project, create new features to split the categories.\n",
    "# df['new_category'] = df.category.str.split(\"/\", expand=False)\n",
    "# split_cat = pd.DataFrame(df['new_category'].tolist(), columns=['category1', 'category2'])\n",
    "# #each project should at least have 1 category, 'category2' can be \"None\". \n",
    "# #'category1' being the main category for the project.\n",
    "# df = pd.concat([df, split_cat], axis=1)\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4222",
   "language": "python",
   "name": "bt4222"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b14d2bd7895077ad303f266db7ad1f8a11e285bbfcdfa868008aad211f623e81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
