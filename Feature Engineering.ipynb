{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfcff7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b667a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Kickstarter_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30a6a68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1404\n",
       "0     728\n",
       "Name: has_video, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'video' to a binary categorical variable\n",
    "df['video'].value_counts()\n",
    "df['has_video'] = df['video'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "df['has_video'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6f8c5",
   "metadata": {},
   "source": [
    "NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1445895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning for: rewards, description, description story, description risks\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    def process_rewards(corpus):\n",
    "    \n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row_processed = \"\"\n",
    "            row = row.replace(\"\\\\n\", \" \")\n",
    "            row = ast.literal_eval(row)\n",
    "\n",
    "            for dict in row:\n",
    "                row_processed += dict['rewards'].lower() + ' '\n",
    "            \n",
    "            \n",
    "            row_processed = row_processed.replace(\"//\",'')\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed)\n",
    "\n",
    "        return corpus_processed\n",
    "    \n",
    "    def process_description_story(corpus):\n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row = str(row)\n",
    "            row_processed = row.replace(\"\\r\", \" \" )\n",
    "            row_processed = row_processed.replace(\"\\n\", \" \" )\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed)\n",
    "\n",
    "        return corpus_processed\n",
    "\n",
    "    df[\"rewards_processed\"] = process_rewards(df[\"rewards\"])\n",
    "    df[\"description_processed\"] = process_description_story(df[\"description\"])\n",
    "    df[\"description_story_processed\"] = process_description_story(df[\"description_story\"])\n",
    "    df[\"description_risks_processed\"] = process_description_story(df[\"description_risks\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f95c3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizeTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62c9ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nlp_features(df):\n",
    "    \n",
    "    # Rewards\n",
    "\n",
    "    vect_rewards = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    rewards_processed = pd.Series(df[\"rewards_processed\"])\n",
    "    tfidf_fit_rewards = vect_rewards.fit(rewards_processed)\n",
    "    rewards_tfidf_array = tfidf_fit_rewards.transform(rewards_processed).toarray()\n",
    "    rewards_tfidf_df = pd.DataFrame(rewards_tfidf_array)\n",
    "    rewards_tfidf_df.columns = list(map(lambda x : \"rewards_\" + str(x), rewards_tfidf_df.columns))\n",
    "    df = pd.merge(df, rewards_tfidf_df , left_index=True, right_index=True)\n",
    "    \n",
    "\n",
    "    # Description\n",
    "\n",
    "    vect_description = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    description = pd.Series(df[\"description_processed\"])\n",
    "    tfidf_fit_description = vect_description.fit(description)\n",
    "    description_tfidf_array = tfidf_fit_description.transform(description).toarray()\n",
    "    description_tfidf_df = pd.DataFrame(description_tfidf_array)\n",
    "    description_tfidf_df.columns = list(map(lambda x : \"description_\" + str(x), description_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Description Story\n",
    "\n",
    "    vect_description_story = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "    \n",
    "    description_story_processed = pd.Series(df[\"description_story_processed\"])\n",
    "    tfidf_fit_description_story_processed = vect_description_story.fit(description_story_processed)\n",
    "    description_story_processed_tfidf_array = tfidf_fit_description_story_processed.transform(description_story_processed).toarray()\n",
    "    description_story_tfidf_df = pd.DataFrame(description_story_processed_tfidf_array)\n",
    "    description_story_tfidf_df.columns = list(map(lambda x : \"description_story_\" + str(x), description_story_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_story_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Description Risks\n",
    "\n",
    "    vect_description_risks = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "\n",
    "    description_risks_processed = pd.Series(df[\"description_risks_processed\"])\n",
    "    tfidf_fit_description_risks_processed = vect_description_risks.fit(description_risks_processed)\n",
    "    description_risks_processed_tfidf_array = tfidf_fit_description_risks_processed.transform(description_risks_processed).toarray()\n",
    "    description_risks_tfidf_df = pd.DataFrame(description_risks_processed_tfidf_array)\n",
    "    description_risks_tfidf_df.columns = list(map(lambda x : \"description_risks_\" + str(x), description_risks_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_risks_tfidf_df , left_index=True, right_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "965ebbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valen\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "df = generate_nlp_features(clean_text(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dae2d5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "name\n",
      "description\n",
      "description_story\n",
      "description_risks\n",
      "rewards\n",
      "category\n",
      "pledged\n",
      "goal\n",
      "deadline\n",
      "location\n",
      "state\n",
      "faq_count\n",
      "update_count\n",
      "backers_count\n",
      "is_starrable\n",
      "spotlight\n",
      "staff_pick\n",
      "video\n",
      "creator_name\n",
      "creator_url\n",
      "url\n",
      "created_at\n",
      "published_at\n",
      "launched_at\n",
      "link\n",
      "has_video\n",
      "rewards_processed\n",
      "description_processed\n",
      "description_story_processed\n",
      "description_risks_processed\n",
      "rewards_0\n",
      "rewards_1\n",
      "rewards_2\n",
      "rewards_3\n",
      "rewards_4\n",
      "rewards_5\n",
      "rewards_6\n",
      "rewards_7\n",
      "rewards_8\n",
      "rewards_9\n",
      "rewards_10\n",
      "rewards_11\n",
      "rewards_12\n",
      "rewards_13\n",
      "rewards_14\n",
      "rewards_15\n",
      "rewards_16\n",
      "rewards_17\n",
      "rewards_18\n",
      "rewards_19\n",
      "rewards_20\n",
      "rewards_21\n",
      "rewards_22\n",
      "rewards_23\n",
      "rewards_24\n",
      "rewards_25\n",
      "rewards_26\n",
      "rewards_27\n",
      "rewards_28\n",
      "rewards_29\n",
      "rewards_30\n",
      "rewards_31\n",
      "rewards_32\n",
      "rewards_33\n",
      "rewards_34\n",
      "rewards_35\n",
      "rewards_36\n",
      "rewards_37\n",
      "rewards_38\n",
      "rewards_39\n",
      "rewards_40\n",
      "rewards_41\n",
      "rewards_42\n",
      "rewards_43\n",
      "rewards_44\n",
      "rewards_45\n",
      "rewards_46\n",
      "rewards_47\n",
      "rewards_48\n",
      "rewards_49\n",
      "rewards_50\n",
      "rewards_51\n",
      "rewards_52\n",
      "rewards_53\n",
      "rewards_54\n",
      "rewards_55\n",
      "rewards_56\n",
      "rewards_57\n",
      "rewards_58\n",
      "rewards_59\n",
      "rewards_60\n",
      "rewards_61\n",
      "rewards_62\n",
      "rewards_63\n",
      "rewards_64\n",
      "rewards_65\n",
      "rewards_66\n",
      "rewards_67\n",
      "rewards_68\n",
      "rewards_69\n",
      "rewards_70\n",
      "rewards_71\n",
      "rewards_72\n",
      "rewards_73\n",
      "rewards_74\n",
      "rewards_75\n",
      "rewards_76\n",
      "rewards_77\n",
      "rewards_78\n",
      "rewards_79\n",
      "rewards_80\n",
      "rewards_81\n",
      "rewards_82\n",
      "rewards_83\n",
      "rewards_84\n",
      "rewards_85\n",
      "rewards_86\n",
      "rewards_87\n",
      "rewards_88\n",
      "rewards_89\n",
      "rewards_90\n",
      "rewards_91\n",
      "rewards_92\n",
      "rewards_93\n",
      "rewards_94\n",
      "rewards_95\n",
      "rewards_96\n",
      "rewards_97\n",
      "rewards_98\n",
      "rewards_99\n",
      "description_0\n",
      "description_1\n",
      "description_2\n",
      "description_3\n",
      "description_4\n",
      "description_5\n",
      "description_6\n",
      "description_7\n",
      "description_8\n",
      "description_9\n",
      "description_10\n",
      "description_11\n",
      "description_12\n",
      "description_13\n",
      "description_14\n",
      "description_15\n",
      "description_16\n",
      "description_17\n",
      "description_18\n",
      "description_19\n",
      "description_20\n",
      "description_21\n",
      "description_22\n",
      "description_23\n",
      "description_24\n",
      "description_25\n",
      "description_26\n",
      "description_27\n",
      "description_28\n",
      "description_29\n",
      "description_30\n",
      "description_31\n",
      "description_32\n",
      "description_33\n",
      "description_34\n",
      "description_35\n",
      "description_36\n",
      "description_37\n",
      "description_38\n",
      "description_39\n",
      "description_40\n",
      "description_41\n",
      "description_42\n",
      "description_43\n",
      "description_44\n",
      "description_45\n",
      "description_46\n",
      "description_47\n",
      "description_48\n",
      "description_49\n",
      "description_50\n",
      "description_51\n",
      "description_52\n",
      "description_53\n",
      "description_54\n",
      "description_55\n",
      "description_56\n",
      "description_57\n",
      "description_58\n",
      "description_59\n",
      "description_60\n",
      "description_61\n",
      "description_62\n",
      "description_63\n",
      "description_64\n",
      "description_65\n",
      "description_66\n",
      "description_67\n",
      "description_68\n",
      "description_69\n",
      "description_70\n",
      "description_71\n",
      "description_72\n",
      "description_73\n",
      "description_74\n",
      "description_75\n",
      "description_76\n",
      "description_77\n",
      "description_78\n",
      "description_79\n",
      "description_80\n",
      "description_81\n",
      "description_82\n",
      "description_83\n",
      "description_84\n",
      "description_85\n",
      "description_86\n",
      "description_87\n",
      "description_88\n",
      "description_89\n",
      "description_90\n",
      "description_91\n",
      "description_92\n",
      "description_93\n",
      "description_94\n",
      "description_95\n",
      "description_96\n",
      "description_97\n",
      "description_98\n",
      "description_99\n",
      "description_story_0\n",
      "description_story_1\n",
      "description_story_2\n",
      "description_story_3\n",
      "description_story_4\n",
      "description_story_5\n",
      "description_story_6\n",
      "description_story_7\n",
      "description_story_8\n",
      "description_story_9\n",
      "description_story_10\n",
      "description_story_11\n",
      "description_story_12\n",
      "description_story_13\n",
      "description_story_14\n",
      "description_story_15\n",
      "description_story_16\n",
      "description_story_17\n",
      "description_story_18\n",
      "description_story_19\n",
      "description_story_20\n",
      "description_story_21\n",
      "description_story_22\n",
      "description_story_23\n",
      "description_story_24\n",
      "description_story_25\n",
      "description_story_26\n",
      "description_story_27\n",
      "description_story_28\n",
      "description_story_29\n",
      "description_story_30\n",
      "description_story_31\n",
      "description_story_32\n",
      "description_story_33\n",
      "description_story_34\n",
      "description_story_35\n",
      "description_story_36\n",
      "description_story_37\n",
      "description_story_38\n",
      "description_story_39\n",
      "description_story_40\n",
      "description_story_41\n",
      "description_story_42\n",
      "description_story_43\n",
      "description_story_44\n",
      "description_story_45\n",
      "description_story_46\n",
      "description_story_47\n",
      "description_story_48\n",
      "description_story_49\n",
      "description_story_50\n",
      "description_story_51\n",
      "description_story_52\n",
      "description_story_53\n",
      "description_story_54\n",
      "description_story_55\n",
      "description_story_56\n",
      "description_story_57\n",
      "description_story_58\n",
      "description_story_59\n",
      "description_story_60\n",
      "description_story_61\n",
      "description_story_62\n",
      "description_story_63\n",
      "description_story_64\n",
      "description_story_65\n",
      "description_story_66\n",
      "description_story_67\n",
      "description_story_68\n",
      "description_story_69\n",
      "description_story_70\n",
      "description_story_71\n",
      "description_story_72\n",
      "description_story_73\n",
      "description_story_74\n",
      "description_story_75\n",
      "description_story_76\n",
      "description_story_77\n",
      "description_story_78\n",
      "description_story_79\n",
      "description_story_80\n",
      "description_story_81\n",
      "description_story_82\n",
      "description_story_83\n",
      "description_story_84\n",
      "description_story_85\n",
      "description_story_86\n",
      "description_story_87\n",
      "description_story_88\n",
      "description_story_89\n",
      "description_story_90\n",
      "description_story_91\n",
      "description_story_92\n",
      "description_story_93\n",
      "description_story_94\n",
      "description_story_95\n",
      "description_story_96\n",
      "description_story_97\n",
      "description_story_98\n",
      "description_story_99\n",
      "description_risks_0\n",
      "description_risks_1\n",
      "description_risks_2\n",
      "description_risks_3\n",
      "description_risks_4\n",
      "description_risks_5\n",
      "description_risks_6\n",
      "description_risks_7\n",
      "description_risks_8\n",
      "description_risks_9\n",
      "description_risks_10\n",
      "description_risks_11\n",
      "description_risks_12\n",
      "description_risks_13\n",
      "description_risks_14\n",
      "description_risks_15\n",
      "description_risks_16\n",
      "description_risks_17\n",
      "description_risks_18\n",
      "description_risks_19\n",
      "description_risks_20\n",
      "description_risks_21\n",
      "description_risks_22\n",
      "description_risks_23\n",
      "description_risks_24\n",
      "description_risks_25\n",
      "description_risks_26\n",
      "description_risks_27\n",
      "description_risks_28\n",
      "description_risks_29\n",
      "description_risks_30\n",
      "description_risks_31\n",
      "description_risks_32\n",
      "description_risks_33\n",
      "description_risks_34\n",
      "description_risks_35\n",
      "description_risks_36\n",
      "description_risks_37\n",
      "description_risks_38\n",
      "description_risks_39\n",
      "description_risks_40\n",
      "description_risks_41\n",
      "description_risks_42\n",
      "description_risks_43\n",
      "description_risks_44\n",
      "description_risks_45\n",
      "description_risks_46\n",
      "description_risks_47\n",
      "description_risks_48\n",
      "description_risks_49\n",
      "description_risks_50\n",
      "description_risks_51\n",
      "description_risks_52\n",
      "description_risks_53\n",
      "description_risks_54\n",
      "description_risks_55\n",
      "description_risks_56\n",
      "description_risks_57\n",
      "description_risks_58\n",
      "description_risks_59\n",
      "description_risks_60\n",
      "description_risks_61\n",
      "description_risks_62\n",
      "description_risks_63\n",
      "description_risks_64\n",
      "description_risks_65\n",
      "description_risks_66\n",
      "description_risks_67\n",
      "description_risks_68\n",
      "description_risks_69\n",
      "description_risks_70\n",
      "description_risks_71\n",
      "description_risks_72\n",
      "description_risks_73\n",
      "description_risks_74\n",
      "description_risks_75\n",
      "description_risks_76\n",
      "description_risks_77\n",
      "description_risks_78\n",
      "description_risks_79\n",
      "description_risks_80\n",
      "description_risks_81\n",
      "description_risks_82\n",
      "description_risks_83\n",
      "description_risks_84\n",
      "description_risks_85\n",
      "description_risks_86\n",
      "description_risks_87\n",
      "description_risks_88\n",
      "description_risks_89\n",
      "description_risks_90\n",
      "description_risks_91\n",
      "description_risks_92\n",
      "description_risks_93\n",
      "description_risks_94\n",
      "description_risks_95\n",
      "description_risks_96\n",
      "description_risks_97\n",
      "description_risks_98\n",
      "description_risks_99\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880f30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77238a471535228e8cd55a3ca9e771a69c6c0bc66c44a56c972f9554a4042742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
