{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24756, 579) ../data/train\\kickstarter_train_final_20221108-144153.csv\n",
      "(4369, 579) ../data/test\\kickstarter_test_final_20221108-144158.csv\n"
     ]
    }
   ],
   "source": [
    "train_path = max(glob.glob('../data/train/*.csv'), key=os.path.getctime) \n",
    "test_path = max(glob.glob('../data/test/*.csv'), key=os.path.getctime) \n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "print(train_df.shape, train_path)\n",
    "print(test_df.shape, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_to_drop = ['rewards', 'deadline', 'launched_at', 'rewards_processed',\n",
    "#            'description_processed', 'description_story_processed','description_risks_processed',\n",
    "#            'id', 'name', 'description', 'description_story', 'description_risks', 'video', 'state',\n",
    "#           'pledged', 'category', 'location']\n",
    "\n",
    "# #features that are dependent on time and the final outcome\n",
    "# to_drop_more = features_to_drop + ['staff_pick', 'spotlight', 'backers_count', 'update_count', 'faq_count']\n",
    "\n",
    "X_train, y_train = train_df.drop('state', axis=1), train_df['state']\n",
    "X_test, y_test = test_df.drop('state', axis=1), test_df['state']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test data set tgt\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17482, 579)\n",
      "(7274, 579)\n",
      "(24756, 578)\n",
      "(24756,)\n",
      "(4369, 578)\n",
      "(4369,)\n"
     ]
    }
   ],
   "source": [
    "# Pretty balanced dataset\n",
    "print(train_df[train_df.state == 1].shape)\n",
    "print(train_df[train_df.state == 0].shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('float64'), dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest model training and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model \n",
    "# training\n",
    "regressor = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "regressor.fit(X_train, y_train)\n",
    "# apply model\n",
    "y_pred_train = regressor.predict(X_train)\n",
    "y_pred_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.28      0.42      7274\n",
      "           1       0.76      0.98      0.86     17482\n",
      "\n",
      "    accuracy                           0.77     24756\n",
      "   macro avg       0.80      0.63      0.64     24756\n",
      "weighted avg       0.79      0.77      0.73     24756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.26      0.40      1282\n",
      "           1       0.76      0.98      0.86      3087\n",
      "\n",
      "    accuracy                           0.77      4369\n",
      "   macro avg       0.80      0.62      0.63      4369\n",
      "weighted avg       0.79      0.77      0.72      4369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**\n",
    "\n",
    "We will use RandomizedSearchCV for hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [50, 100, 150, 200], 'max_features': ['auto', 'sqrt'], 'max_depth': [3, 5, 8, 10], 'min_samples_split': [3, 5, 8, 10], 'min_samples_leaf': [2, 4, 6], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [3,5,8, 10]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [3, 5, 8, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [ 2, 4, 6]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 100 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=2022),\n",
       "                   estimator=RandomForestClassifier(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [3, 5, 8, 10],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [2, 4, 6],\n",
       "                                        'min_samples_split': [3, 5, 8, 10],\n",
       "                                        'n_estimators': [50, 100, 150, 200]},\n",
       "                   random_state=2022, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model \n",
    "# training\n",
    "regressor = RandomForestClassifier()\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=2022)\n",
    "\n",
    "rf_randomcv = RandomizedSearchCV(\n",
    "    estimator=regressor,\n",
    "    param_distributions=grid,\n",
    "    n_iter=100, \n",
    "    cv=cv_method,\n",
    "    verbose=2,\n",
    "    random_state=2022,\n",
    "    n_jobs=-1, # use all processors,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "rf_randomcv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_ = {'n_estimators': 150,\n",
    " 'min_samples_split': 5,\n",
    " 'min_samples_leaf': 6,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_depth': 10,\n",
    " 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.873366168258555"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use tuned classifier on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for the program execution 39.6793999671936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.83727   0.46958   0.60170      1282\n",
      "           1    0.81370   0.96210   0.88170      3087\n",
      "\n",
      "    accuracy                        0.81758      4369\n",
      "   macro avg    0.82549   0.71584   0.74170      4369\n",
      "weighted avg    0.82062   0.81758   0.79954      4369\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8840851398876168"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# best_clf = RandomForestClassifier(**rf_randomcv.best_params_)\n",
    "best_clf = RandomForestClassifier(**best_params_)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_clf.fit(X_train, y_train)\n",
    "y_pred_test = best_clf.predict(X_test)\n",
    "y_pred_test_proba = best_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds\n",
    "print(classification_report(y_test, y_pred_test, digits=5))\n",
    "roc_auc_score(y_test, y_pred_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking:\n",
      "1. features 421 (0.104366)\n",
      "2. features 4 (0.081595)\n",
      "3. features 0 (0.069119)\n",
      "4. features 3 (0.043552)\n",
      "5. features 414 (0.040231)\n",
      "6. features 418 (0.028688)\n",
      "7. features 247 (0.026502)\n",
      "8. features 5 (0.022260)\n",
      "9. features 61 (0.020921)\n",
      "10. features 416 (0.020791)\n",
      "11. features 417 (0.018283)\n",
      "12. features 274 (0.016516)\n",
      "13. features 2 (0.016187)\n",
      "14. features 219 (0.013324)\n",
      "15. features 211 (0.012084)\n",
      "16. features 71 (0.011113)\n",
      "17. features 102 (0.011077)\n",
      "18. features 300 (0.009621)\n",
      "19. features 1 (0.009318)\n",
      "20. features 47 (0.009218)\n",
      "21. features 30 (0.008773)\n",
      "22. features 301 (0.008680)\n",
      "23. features 60 (0.008204)\n",
      "24. features 244 (0.008041)\n",
      "25. features 305 (0.007711)\n",
      "26. features 81 (0.007339)\n",
      "27. features 95 (0.007157)\n",
      "28. features 105 (0.006510)\n",
      "29. features 262 (0.006418)\n",
      "30. features 293 (0.005987)\n",
      "31. features 10 (0.005671)\n",
      "32. features 66 (0.005602)\n",
      "33. features 353 (0.005534)\n",
      "34. features 215 (0.005093)\n",
      "35. features 212 (0.005021)\n",
      "36. features 76 (0.005000)\n",
      "37. features 8 (0.004424)\n",
      "38. features 87 (0.004329)\n",
      "39. features 285 (0.004298)\n",
      "40. features 100 (0.004255)\n",
      "41. features 415 (0.004218)\n",
      "42. features 28 (0.004217)\n",
      "43. features 45 (0.004016)\n",
      "44. features 422 (0.004005)\n",
      "45. features 268 (0.003968)\n",
      "46. features 431 (0.003965)\n",
      "47. features 9 (0.003952)\n",
      "48. features 74 (0.003948)\n",
      "49. features 292 (0.003781)\n",
      "50. features 235 (0.003732)\n",
      "51. features 48 (0.003643)\n",
      "52. features 272 (0.003586)\n",
      "53. features 88 (0.003296)\n",
      "54. features 101 (0.003252)\n",
      "55. features 7 (0.003184)\n",
      "56. features 22 (0.003136)\n",
      "57. features 50 (0.003109)\n",
      "58. features 25 (0.003057)\n",
      "59. features 419 (0.003019)\n",
      "60. features 82 (0.002958)\n",
      "61. features 63 (0.002951)\n",
      "62. features 38 (0.002910)\n",
      "63. features 64 (0.002853)\n",
      "64. features 83 (0.002785)\n",
      "65. features 72 (0.002755)\n",
      "66. features 65 (0.002752)\n",
      "67. features 12 (0.002653)\n",
      "68. features 49 (0.002600)\n",
      "69. features 29 (0.002556)\n",
      "70. features 24 (0.002358)\n",
      "71. features 265 (0.002324)\n",
      "72. features 406 (0.002316)\n",
      "73. features 78 (0.002269)\n",
      "74. features 46 (0.002268)\n",
      "75. features 407 (0.002250)\n",
      "76. features 269 (0.002193)\n",
      "77. features 103 (0.002172)\n",
      "78. features 412 (0.002128)\n",
      "79. features 413 (0.002121)\n",
      "80. features 96 (0.002083)\n",
      "81. features 13 (0.002019)\n",
      "82. features 23 (0.001981)\n",
      "83. features 316 (0.001912)\n",
      "84. features 299 (0.001900)\n",
      "85. features 290 (0.001834)\n",
      "86. features 236 (0.001793)\n",
      "87. features 411 (0.001736)\n",
      "88. features 388 (0.001712)\n",
      "89. features 263 (0.001704)\n",
      "90. features 226 (0.001684)\n",
      "91. features 14 (0.001681)\n",
      "92. features 420 (0.001618)\n",
      "93. features 73 (0.001613)\n",
      "94. features 410 (0.001608)\n",
      "95. features 278 (0.001602)\n",
      "96. features 254 (0.001592)\n",
      "97. features 304 (0.001578)\n",
      "98. features 255 (0.001573)\n",
      "99. features 317 (0.001546)\n",
      "100. features 97 (0.001539)\n",
      "101. features 261 (0.001456)\n",
      "102. features 58 (0.001440)\n",
      "103. features 93 (0.001431)\n",
      "104. features 6 (0.001427)\n",
      "105. features 15 (0.001401)\n",
      "106. features 297 (0.001376)\n",
      "107. features 276 (0.001354)\n",
      "108. features 251 (0.001332)\n",
      "109. features 303 (0.001330)\n",
      "110. features 239 (0.001306)\n",
      "111. features 296 (0.001294)\n",
      "112. features 92 (0.001289)\n",
      "113. features 409 (0.001275)\n",
      "114. features 245 (0.001256)\n",
      "115. features 408 (0.001256)\n",
      "116. features 77 (0.001252)\n",
      "117. features 424 (0.001238)\n",
      "118. features 85 (0.001230)\n",
      "119. features 213 (0.001221)\n",
      "120. features 209 (0.001219)\n",
      "121. features 84 (0.001194)\n",
      "122. features 233 (0.001187)\n",
      "123. features 80 (0.001179)\n",
      "124. features 89 (0.001167)\n",
      "125. features 35 (0.001166)\n",
      "126. features 252 (0.001140)\n",
      "127. features 400 (0.001130)\n",
      "128. features 33 (0.001129)\n",
      "129. features 311 (0.001126)\n",
      "130. features 98 (0.001124)\n",
      "131. features 264 (0.001073)\n",
      "132. features 259 (0.001068)\n",
      "133. features 271 (0.001067)\n",
      "134. features 34 (0.001048)\n",
      "135. features 385 (0.001022)\n",
      "136. features 222 (0.001020)\n",
      "137. features 232 (0.001012)\n",
      "138. features 326 (0.001011)\n",
      "139. features 221 (0.001005)\n",
      "140. features 242 (0.001004)\n",
      "141. features 256 (0.000991)\n",
      "142. features 27 (0.000990)\n",
      "143. features 216 (0.000985)\n",
      "144. features 270 (0.000982)\n",
      "145. features 214 (0.000960)\n",
      "146. features 260 (0.000933)\n",
      "147. features 208 (0.000932)\n",
      "148. features 220 (0.000930)\n",
      "149. features 295 (0.000921)\n",
      "150. features 94 (0.000904)\n",
      "151. features 99 (0.000898)\n",
      "152. features 237 (0.000881)\n",
      "153. features 298 (0.000880)\n",
      "154. features 253 (0.000859)\n",
      "155. features 286 (0.000855)\n",
      "156. features 223 (0.000849)\n",
      "157. features 428 (0.000845)\n",
      "158. features 257 (0.000840)\n",
      "159. features 404 (0.000838)\n",
      "160. features 210 (0.000829)\n",
      "161. features 240 (0.000810)\n",
      "162. features 68 (0.000774)\n",
      "163. features 294 (0.000770)\n",
      "164. features 227 (0.000770)\n",
      "165. features 258 (0.000762)\n",
      "166. features 230 (0.000744)\n",
      "167. features 90 (0.000740)\n",
      "168. features 283 (0.000739)\n",
      "169. features 207 (0.000739)\n",
      "170. features 288 (0.000728)\n",
      "171. features 57 (0.000707)\n",
      "172. features 241 (0.000707)\n",
      "173. features 267 (0.000698)\n",
      "174. features 379 (0.000683)\n",
      "175. features 16 (0.000674)\n",
      "176. features 91 (0.000671)\n",
      "177. features 275 (0.000668)\n",
      "178. features 229 (0.000657)\n",
      "179. features 378 (0.000645)\n",
      "180. features 218 (0.000638)\n",
      "181. features 248 (0.000621)\n",
      "182. features 302 (0.000602)\n",
      "183. features 107 (0.000600)\n",
      "184. features 37 (0.000593)\n",
      "185. features 429 (0.000588)\n",
      "186. features 26 (0.000583)\n",
      "187. features 231 (0.000574)\n",
      "188. features 289 (0.000571)\n",
      "189. features 20 (0.000566)\n",
      "190. features 284 (0.000563)\n",
      "191. features 243 (0.000562)\n",
      "192. features 86 (0.000560)\n",
      "193. features 234 (0.000554)\n",
      "194. features 266 (0.000554)\n",
      "195. features 52 (0.000552)\n",
      "196. features 21 (0.000551)\n",
      "197. features 75 (0.000528)\n",
      "198. features 40 (0.000520)\n",
      "199. features 36 (0.000520)\n",
      "200. features 277 (0.000518)\n",
      "201. features 282 (0.000517)\n",
      "202. features 224 (0.000517)\n",
      "203. features 280 (0.000503)\n",
      "204. features 104 (0.000502)\n",
      "205. features 340 (0.000488)\n",
      "206. features 291 (0.000485)\n",
      "207. features 279 (0.000483)\n",
      "208. features 348 (0.000483)\n",
      "209. features 59 (0.000478)\n",
      "210. features 395 (0.000474)\n",
      "211. features 228 (0.000474)\n",
      "212. features 281 (0.000468)\n",
      "213. features 17 (0.000460)\n",
      "214. features 372 (0.000456)\n",
      "215. features 482 (0.000446)\n",
      "216. features 246 (0.000444)\n",
      "217. features 341 (0.000441)\n",
      "218. features 347 (0.000436)\n",
      "219. features 309 (0.000427)\n",
      "220. features 67 (0.000427)\n",
      "221. features 53 (0.000426)\n",
      "222. features 176 (0.000416)\n",
      "223. features 383 (0.000406)\n",
      "224. features 367 (0.000406)\n",
      "225. features 396 (0.000405)\n",
      "226. features 344 (0.000401)\n",
      "227. features 206 (0.000394)\n",
      "228. features 369 (0.000388)\n",
      "229. features 320 (0.000387)\n",
      "230. features 337 (0.000377)\n",
      "231. features 342 (0.000375)\n",
      "232. features 359 (0.000368)\n",
      "233. features 363 (0.000361)\n",
      "234. features 389 (0.000352)\n",
      "235. features 313 (0.000342)\n",
      "236. features 41 (0.000340)\n",
      "237. features 39 (0.000339)\n",
      "238. features 31 (0.000337)\n",
      "239. features 376 (0.000325)\n",
      "240. features 125 (0.000324)\n",
      "241. features 173 (0.000315)\n",
      "242. features 42 (0.000311)\n",
      "243. features 390 (0.000310)\n",
      "244. features 401 (0.000308)\n",
      "245. features 287 (0.000307)\n",
      "246. features 315 (0.000307)\n",
      "247. features 225 (0.000303)\n",
      "248. features 373 (0.000300)\n",
      "249. features 249 (0.000290)\n",
      "250. features 377 (0.000287)\n",
      "251. features 108 (0.000285)\n",
      "252. features 19 (0.000283)\n",
      "253. features 186 (0.000282)\n",
      "254. features 69 (0.000279)\n",
      "255. features 349 (0.000278)\n",
      "256. features 109 (0.000278)\n",
      "257. features 79 (0.000276)\n",
      "258. features 62 (0.000275)\n",
      "259. features 392 (0.000275)\n",
      "260. features 44 (0.000274)\n",
      "261. features 51 (0.000268)\n",
      "262. features 11 (0.000266)\n",
      "263. features 386 (0.000266)\n",
      "264. features 336 (0.000263)\n",
      "265. features 308 (0.000263)\n",
      "266. features 436 (0.000263)\n",
      "267. features 397 (0.000262)\n",
      "268. features 331 (0.000262)\n",
      "269. features 398 (0.000260)\n",
      "270. features 371 (0.000257)\n",
      "271. features 393 (0.000256)\n",
      "272. features 217 (0.000255)\n",
      "273. features 351 (0.000253)\n",
      "274. features 343 (0.000253)\n",
      "275. features 435 (0.000248)\n",
      "276. features 250 (0.000248)\n",
      "277. features 55 (0.000248)\n",
      "278. features 306 (0.000245)\n",
      "279. features 360 (0.000243)\n",
      "280. features 150 (0.000243)\n",
      "281. features 318 (0.000243)\n",
      "282. features 70 (0.000242)\n",
      "283. features 405 (0.000241)\n",
      "284. features 156 (0.000239)\n",
      "285. features 54 (0.000237)\n",
      "286. features 43 (0.000233)\n",
      "287. features 356 (0.000231)\n",
      "288. features 364 (0.000228)\n",
      "289. features 432 (0.000227)\n",
      "290. features 365 (0.000217)\n",
      "291. features 381 (0.000215)\n",
      "292. features 362 (0.000208)\n",
      "293. features 56 (0.000208)\n",
      "294. features 123 (0.000205)\n",
      "295. features 321 (0.000203)\n",
      "296. features 374 (0.000199)\n",
      "297. features 330 (0.000197)\n",
      "298. features 361 (0.000197)\n",
      "299. features 18 (0.000192)\n",
      "300. features 273 (0.000192)\n",
      "301. features 430 (0.000191)\n",
      "302. features 368 (0.000190)\n",
      "303. features 124 (0.000190)\n",
      "304. features 354 (0.000188)\n",
      "305. features 322 (0.000188)\n",
      "306. features 567 (0.000187)\n",
      "307. features 32 (0.000184)\n",
      "308. features 175 (0.000183)\n",
      "309. features 399 (0.000182)\n",
      "310. features 199 (0.000178)\n",
      "311. features 375 (0.000173)\n",
      "312. features 185 (0.000169)\n",
      "313. features 394 (0.000169)\n",
      "314. features 238 (0.000169)\n",
      "315. features 402 (0.000169)\n",
      "316. features 339 (0.000167)\n",
      "317. features 327 (0.000165)\n",
      "318. features 332 (0.000164)\n",
      "319. features 334 (0.000162)\n",
      "320. features 366 (0.000159)\n",
      "321. features 384 (0.000159)\n",
      "322. features 370 (0.000158)\n",
      "323. features 157 (0.000157)\n",
      "324. features 382 (0.000154)\n",
      "325. features 346 (0.000149)\n",
      "326. features 135 (0.000148)\n",
      "327. features 345 (0.000147)\n",
      "328. features 338 (0.000146)\n",
      "329. features 324 (0.000145)\n",
      "330. features 387 (0.000142)\n",
      "331. features 358 (0.000139)\n",
      "332. features 355 (0.000139)\n",
      "333. features 333 (0.000138)\n",
      "334. features 325 (0.000133)\n",
      "335. features 312 (0.000133)\n",
      "336. features 319 (0.000131)\n",
      "337. features 307 (0.000131)\n",
      "338. features 323 (0.000130)\n",
      "339. features 403 (0.000128)\n",
      "340. features 191 (0.000128)\n",
      "341. features 328 (0.000127)\n",
      "342. features 380 (0.000127)\n",
      "343. features 391 (0.000124)\n",
      "344. features 335 (0.000123)\n",
      "345. features 183 (0.000123)\n",
      "346. features 352 (0.000122)\n",
      "347. features 198 (0.000121)\n",
      "348. features 203 (0.000121)\n",
      "349. features 329 (0.000120)\n",
      "350. features 526 (0.000115)\n",
      "351. features 144 (0.000114)\n",
      "352. features 350 (0.000111)\n",
      "353. features 310 (0.000109)\n",
      "354. features 137 (0.000099)\n",
      "355. features 159 (0.000098)\n",
      "356. features 170 (0.000094)\n",
      "357. features 196 (0.000093)\n",
      "358. features 427 (0.000092)\n",
      "359. features 357 (0.000088)\n",
      "360. features 130 (0.000086)\n",
      "361. features 165 (0.000085)\n",
      "362. features 500 (0.000084)\n",
      "363. features 168 (0.000084)\n",
      "364. features 167 (0.000081)\n",
      "365. features 119 (0.000081)\n",
      "366. features 177 (0.000079)\n",
      "367. features 178 (0.000073)\n",
      "368. features 158 (0.000070)\n",
      "369. features 138 (0.000069)\n",
      "370. features 134 (0.000069)\n",
      "371. features 425 (0.000069)\n",
      "372. features 151 (0.000067)\n",
      "373. features 113 (0.000064)\n",
      "374. features 434 (0.000064)\n",
      "375. features 314 (0.000061)\n",
      "376. features 128 (0.000058)\n",
      "377. features 140 (0.000057)\n",
      "378. features 197 (0.000056)\n",
      "379. features 115 (0.000055)\n",
      "380. features 146 (0.000054)\n",
      "381. features 171 (0.000054)\n",
      "382. features 190 (0.000052)\n",
      "383. features 164 (0.000052)\n",
      "384. features 112 (0.000051)\n",
      "385. features 163 (0.000051)\n",
      "386. features 426 (0.000047)\n",
      "387. features 548 (0.000045)\n",
      "388. features 154 (0.000045)\n",
      "389. features 110 (0.000044)\n",
      "390. features 204 (0.000044)\n",
      "391. features 133 (0.000044)\n",
      "392. features 145 (0.000043)\n",
      "393. features 200 (0.000043)\n",
      "394. features 192 (0.000041)\n",
      "395. features 179 (0.000040)\n",
      "396. features 470 (0.000038)\n",
      "397. features 129 (0.000038)\n",
      "398. features 162 (0.000036)\n",
      "399. features 194 (0.000033)\n",
      "400. features 433 (0.000032)\n",
      "401. features 131 (0.000031)\n",
      "402. features 152 (0.000031)\n",
      "403. features 118 (0.000030)\n",
      "404. features 481 (0.000026)\n",
      "405. features 153 (0.000026)\n",
      "406. features 161 (0.000025)\n",
      "407. features 503 (0.000025)\n",
      "408. features 457 (0.000025)\n",
      "409. features 149 (0.000025)\n",
      "410. features 122 (0.000024)\n",
      "411. features 172 (0.000024)\n",
      "412. features 193 (0.000023)\n",
      "413. features 116 (0.000023)\n",
      "414. features 143 (0.000022)\n",
      "415. features 127 (0.000022)\n",
      "416. features 136 (0.000022)\n",
      "417. features 444 (0.000021)\n",
      "418. features 117 (0.000020)\n",
      "419. features 532 (0.000019)\n",
      "420. features 184 (0.000019)\n",
      "421. features 188 (0.000017)\n",
      "422. features 166 (0.000015)\n",
      "423. features 160 (0.000014)\n",
      "424. features 189 (0.000013)\n",
      "425. features 202 (0.000011)\n",
      "426. features 489 (0.000010)\n",
      "427. features 148 (0.000008)\n",
      "428. features 132 (0.000008)\n",
      "429. features 181 (0.000008)\n",
      "430. features 472 (0.000007)\n",
      "431. features 205 (0.000007)\n",
      "432. features 155 (0.000006)\n",
      "433. features 423 (0.000006)\n",
      "434. features 535 (0.000006)\n",
      "435. features 180 (0.000006)\n",
      "436. features 477 (0.000005)\n",
      "437. features 174 (0.000005)\n",
      "438. features 187 (0.000004)\n",
      "439. features 120 (0.000004)\n",
      "440. features 141 (0.000003)\n",
      "441. features 126 (0.000003)\n",
      "442. features 114 (0.000003)\n",
      "443. features 106 (0.000002)\n",
      "444. features 139 (0.000002)\n",
      "445. features 195 (0.000001)\n",
      "446. features 549 (0.000000)\n",
      "447. features 455 (0.000000)\n",
      "448. features 454 (0.000000)\n",
      "449. features 453 (0.000000)\n",
      "450. features 147 (0.000000)\n",
      "451. features 452 (0.000000)\n",
      "452. features 451 (0.000000)\n",
      "453. features 576 (0.000000)\n",
      "454. features 547 (0.000000)\n",
      "455. features 550 (0.000000)\n",
      "456. features 450 (0.000000)\n",
      "457. features 449 (0.000000)\n",
      "458. features 551 (0.000000)\n",
      "459. features 456 (0.000000)\n",
      "460. features 458 (0.000000)\n",
      "461. features 546 (0.000000)\n",
      "462. features 464 (0.000000)\n",
      "463. features 540 (0.000000)\n",
      "464. features 466 (0.000000)\n",
      "465. features 541 (0.000000)\n",
      "466. features 542 (0.000000)\n",
      "467. features 465 (0.000000)\n",
      "468. features 543 (0.000000)\n",
      "469. features 463 (0.000000)\n",
      "470. features 545 (0.000000)\n",
      "471. features 462 (0.000000)\n",
      "472. features 461 (0.000000)\n",
      "473. features 460 (0.000000)\n",
      "474. features 459 (0.000000)\n",
      "475. features 553 (0.000000)\n",
      "476. features 544 (0.000000)\n",
      "477. features 552 (0.000000)\n",
      "478. features 556 (0.000000)\n",
      "479. features 448 (0.000000)\n",
      "480. features 562 (0.000000)\n",
      "481. features 575 (0.000000)\n",
      "482. features 574 (0.000000)\n",
      "483. features 573 (0.000000)\n",
      "484. features 572 (0.000000)\n",
      "485. features 571 (0.000000)\n",
      "486. features 570 (0.000000)\n",
      "487. features 569 (0.000000)\n",
      "488. features 568 (0.000000)\n",
      "489. features 437 (0.000000)\n",
      "490. features 566 (0.000000)\n",
      "491. features 438 (0.000000)\n",
      "492. features 565 (0.000000)\n",
      "493. features 564 (0.000000)\n",
      "494. features 563 (0.000000)\n",
      "495. features 439 (0.000000)\n",
      "496. features 447 (0.000000)\n",
      "497. features 440 (0.000000)\n",
      "498. features 441 (0.000000)\n",
      "499. features 442 (0.000000)\n",
      "500. features 443 (0.000000)\n",
      "501. features 445 (0.000000)\n",
      "502. features 561 (0.000000)\n",
      "503. features 560 (0.000000)\n",
      "504. features 559 (0.000000)\n",
      "505. features 558 (0.000000)\n",
      "506. features 446 (0.000000)\n",
      "507. features 557 (0.000000)\n",
      "508. features 538 (0.000000)\n",
      "509. features 555 (0.000000)\n",
      "510. features 554 (0.000000)\n",
      "511. features 539 (0.000000)\n",
      "512. features 534 (0.000000)\n",
      "513. features 537 (0.000000)\n",
      "514. features 492 (0.000000)\n",
      "515. features 507 (0.000000)\n",
      "516. features 478 (0.000000)\n",
      "517. features 506 (0.000000)\n",
      "518. features 479 (0.000000)\n",
      "519. features 480 (0.000000)\n",
      "520. features 483 (0.000000)\n",
      "521. features 484 (0.000000)\n",
      "522. features 111 (0.000000)\n",
      "523. features 485 (0.000000)\n",
      "524. features 486 (0.000000)\n",
      "525. features 487 (0.000000)\n",
      "526. features 488 (0.000000)\n",
      "527. features 490 (0.000000)\n",
      "528. features 491 (0.000000)\n",
      "529. features 121 (0.000000)\n",
      "530. features 467 (0.000000)\n",
      "531. features 493 (0.000000)\n",
      "532. features 494 (0.000000)\n",
      "533. features 495 (0.000000)\n",
      "534. features 496 (0.000000)\n",
      "535. features 497 (0.000000)\n",
      "536. features 498 (0.000000)\n",
      "537. features 499 (0.000000)\n",
      "538. features 501 (0.000000)\n",
      "539. features 502 (0.000000)\n",
      "540. features 504 (0.000000)\n",
      "541. features 505 (0.000000)\n",
      "542. features 201 (0.000000)\n",
      "543. features 182 (0.000000)\n",
      "544. features 142 (0.000000)\n",
      "545. features 508 (0.000000)\n",
      "546. features 509 (0.000000)\n",
      "547. features 510 (0.000000)\n",
      "548. features 511 (0.000000)\n",
      "549. features 536 (0.000000)\n",
      "550. features 468 (0.000000)\n",
      "551. features 469 (0.000000)\n",
      "552. features 169 (0.000000)\n",
      "553. features 533 (0.000000)\n",
      "554. features 471 (0.000000)\n",
      "555. features 473 (0.000000)\n",
      "556. features 531 (0.000000)\n",
      "557. features 530 (0.000000)\n",
      "558. features 529 (0.000000)\n",
      "559. features 474 (0.000000)\n",
      "560. features 528 (0.000000)\n",
      "561. features 527 (0.000000)\n",
      "562. features 475 (0.000000)\n",
      "563. features 525 (0.000000)\n",
      "564. features 524 (0.000000)\n",
      "565. features 523 (0.000000)\n",
      "566. features 476 (0.000000)\n",
      "567. features 522 (0.000000)\n",
      "568. features 521 (0.000000)\n",
      "569. features 520 (0.000000)\n",
      "570. features 519 (0.000000)\n",
      "571. features 518 (0.000000)\n",
      "572. features 517 (0.000000)\n",
      "573. features 516 (0.000000)\n",
      "574. features 515 (0.000000)\n",
      "575. features 514 (0.000000)\n",
      "576. features 513 (0.000000)\n",
      "577. features 512 (0.000000)\n",
      "578. features 577 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = best_clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_clf.estimators_], axis = 0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print('Feature Ranking:')\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "\tprint('%d. features %d (%f)'% (f+1, indices[f], importances[indices[f]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "77238a471535228e8cd55a3ca9e771a69c6c0bc66c44a56c972f9554a4042742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
