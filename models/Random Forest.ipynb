{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24756, 576)\n",
      "(4369, 576)\n"
     ]
    }
   ],
   "source": [
    "train_path = max(glob.glob('./data/train/*.csv'), key=os.path.getctime) \n",
    "test_path = max(glob.glob('./data/test/*.csv'), key=os.path.getctime) \n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_to_drop = ['rewards', 'deadline', 'launched_at', 'rewards_processed',\n",
    "#            'description_processed', 'description_story_processed','description_risks_processed',\n",
    "#            'id', 'name', 'description', 'description_story', 'description_risks', 'video', 'state',\n",
    "#           'pledged', 'category', 'location']\n",
    "\n",
    "# #features that are dependent on time and the final outcome\n",
    "# to_drop_more = features_to_drop + ['staff_pick', 'spotlight', 'backers_count', 'update_count', 'faq_count']\n",
    "\n",
    "X_train, y_train = train_df.drop('state', axis=1), train_df['state']\n",
    "X_test, y_test = test_df.drop('state', axis=1), test_df['state']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test data set tgt\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17482, 576)\n",
      "(7274, 576)\n",
      "(24756, 575)\n",
      "(24756,)\n",
      "(4369, 575)\n",
      "(4369,)\n"
     ]
    }
   ],
   "source": [
    "# Pretty balanced dataset\n",
    "print(train_df[train_df.state == 1].shape)\n",
    "print(train_df[train_df.state == 0].shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('float64'), dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest model training and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model \n",
    "# training\n",
    "regressor = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "regressor.fit(X_train, y_train)\n",
    "# apply model\n",
    "y_pred_train = regressor.predict(X_train)\n",
    "y_pred_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.67      0.77      7274\n",
      "           1       0.88      0.97      0.92     17482\n",
      "\n",
      "    accuracy                           0.88     24756\n",
      "   macro avg       0.89      0.82      0.85     24756\n",
      "weighted avg       0.89      0.88      0.88     24756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.67      0.77      1282\n",
      "           1       0.88      0.97      0.92      3087\n",
      "\n",
      "    accuracy                           0.88      4369\n",
      "   macro avg       0.89      0.82      0.84      4369\n",
      "weighted avg       0.88      0.88      0.88      4369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**\n",
    "\n",
    "We will use RandomizedSearchCV for hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [50, 100, 150], 'max_features': ['auto', 'sqrt'], 'max_depth': [3, 5, 8], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [50, 100, 150]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [3,5,8]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 100 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=2022),\n",
       "                   estimator=RandomForestClassifier(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [3, 5, 8],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [50, 100, 150]},\n",
       "                   random_state=2022, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model \n",
    "# training\n",
    "regressor = RandomForestClassifier()\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=2022)\n",
    "\n",
    "rf_randomcv = RandomizedSearchCV(\n",
    "    estimator=regressor,\n",
    "    param_distributions=grid,\n",
    "    n_iter=100, \n",
    "    cv=cv_method,\n",
    "    verbose=2,\n",
    "    random_state=2022,\n",
    "    n_jobs=-1, # use all processors,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "rf_randomcv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 150,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 8,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9321799731140534"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use tuned classifier on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for the program execution 22.117509126663208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.89881   0.70671   0.79127      1282\n",
      "           1    0.88813   0.96696   0.92587      3087\n",
      "\n",
      "    accuracy                        0.89059      4369\n",
      "   macro avg    0.89347   0.83683   0.85857      4369\n",
      "weighted avg    0.89126   0.89059   0.88637      4369\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8368332400934522"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "best_clf = RandomForestClassifier(**rf_randomcv.best_params_)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_clf.fit(X_train, y_train)\n",
    "y_pred_test = best_clf.predict(X_test)\n",
    "\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds\n",
    "print(classification_report(y_test, y_pred_test, digits=5))\n",
    "roc_auc_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking:\n",
      "1. features 7 (0.144888)\n",
      "2. features 6 (0.120506)\n",
      "3. features 9 (0.074819)\n",
      "4. features 104 (0.039424)\n",
      "5. features 105 (0.039374)\n",
      "6. features 0 (0.036968)\n",
      "7. features 8 (0.035172)\n",
      "8. features 4 (0.034918)\n",
      "9. features 416 (0.034769)\n",
      "10. features 37 (0.029524)\n",
      "11. features 83 (0.025880)\n",
      "12. features 29 (0.020267)\n",
      "13. features 28 (0.019807)\n",
      "14. features 3 (0.019192)\n",
      "15. features 70 (0.019117)\n",
      "16. features 71 (0.018827)\n",
      "17. features 82 (0.018685)\n",
      "18. features 38 (0.014802)\n",
      "19. features 81 (0.013164)\n",
      "20. features 15 (0.012097)\n",
      "21. features 414 (0.010822)\n",
      "22. features 30 (0.009962)\n",
      "23. features 247 (0.009832)\n",
      "24. features 32 (0.007303)\n",
      "25. features 274 (0.007183)\n",
      "26. features 418 (0.007116)\n",
      "27. features 2 (0.006553)\n",
      "28. features 41 (0.005803)\n",
      "29. features 11 (0.005613)\n",
      "30. features 211 (0.005458)\n",
      "31. features 35 (0.005232)\n",
      "32. features 417 (0.005079)\n",
      "33. features 64 (0.004808)\n",
      "34. features 103 (0.004335)\n",
      "35. features 88 (0.003700)\n",
      "36. features 12 (0.003643)\n",
      "37. features 36 (0.003623)\n",
      "38. features 16 (0.003478)\n",
      "39. features 5 (0.003444)\n",
      "40. features 10 (0.003162)\n",
      "41. features 219 (0.003094)\n",
      "42. features 87 (0.002946)\n",
      "43. features 1 (0.002786)\n",
      "44. features 300 (0.002744)\n",
      "45. features 14 (0.002685)\n",
      "46. features 40 (0.002597)\n",
      "47. features 69 (0.002589)\n",
      "48. features 75 (0.002415)\n",
      "49. features 86 (0.002368)\n",
      "50. features 68 (0.002326)\n",
      "51. features 301 (0.002187)\n",
      "52. features 235 (0.001997)\n",
      "53. features 305 (0.001941)\n",
      "54. features 419 (0.001898)\n",
      "55. features 292 (0.001699)\n",
      "56. features 63 (0.001697)\n",
      "57. features 79 (0.001663)\n",
      "58. features 272 (0.001640)\n",
      "59. features 98 (0.001562)\n",
      "60. features 42 (0.001449)\n",
      "61. features 53 (0.001388)\n",
      "62. features 55 (0.001378)\n",
      "63. features 262 (0.001335)\n",
      "64. features 13 (0.001313)\n",
      "65. features 353 (0.001266)\n",
      "66. features 212 (0.001229)\n",
      "67. features 268 (0.001163)\n",
      "68. features 66 (0.001153)\n",
      "69. features 293 (0.001137)\n",
      "70. features 57 (0.001043)\n",
      "71. features 56 (0.000980)\n",
      "72. features 67 (0.000924)\n",
      "73. features 244 (0.000911)\n",
      "74. features 31 (0.000879)\n",
      "75. features 39 (0.000849)\n",
      "76. features 239 (0.000823)\n",
      "77. features 49 (0.000816)\n",
      "78. features 65 (0.000797)\n",
      "79. features 428 (0.000769)\n",
      "80. features 215 (0.000747)\n",
      "81. features 102 (0.000703)\n",
      "82. features 285 (0.000696)\n",
      "83. features 92 (0.000677)\n",
      "84. features 415 (0.000671)\n",
      "85. features 76 (0.000664)\n",
      "86. features 27 (0.000658)\n",
      "87. features 254 (0.000654)\n",
      "88. features 264 (0.000643)\n",
      "89. features 242 (0.000640)\n",
      "90. features 93 (0.000605)\n",
      "91. features 290 (0.000560)\n",
      "92. features 388 (0.000552)\n",
      "93. features 265 (0.000544)\n",
      "94. features 173 (0.000543)\n",
      "95. features 413 (0.000507)\n",
      "96. features 54 (0.000504)\n",
      "97. features 260 (0.000503)\n",
      "98. features 269 (0.000494)\n",
      "99. features 94 (0.000488)\n",
      "100. features 77 (0.000487)\n",
      "101. features 407 (0.000478)\n",
      "102. features 316 (0.000475)\n",
      "103. features 311 (0.000462)\n",
      "104. features 255 (0.000449)\n",
      "105. features 412 (0.000437)\n",
      "106. features 78 (0.000421)\n",
      "107. features 271 (0.000410)\n",
      "108. features 406 (0.000399)\n",
      "109. features 73 (0.000372)\n",
      "110. features 90 (0.000367)\n",
      "111. features 18 (0.000364)\n",
      "112. features 302 (0.000351)\n",
      "113. features 222 (0.000341)\n",
      "114. features 236 (0.000338)\n",
      "115. features 34 (0.000334)\n",
      "116. features 226 (0.000331)\n",
      "117. features 99 (0.000329)\n",
      "118. features 410 (0.000329)\n",
      "119. features 208 (0.000328)\n",
      "120. features 20 (0.000318)\n",
      "121. features 100 (0.000315)\n",
      "122. features 19 (0.000311)\n",
      "123. features 421 (0.000308)\n",
      "124. features 411 (0.000307)\n",
      "125. features 216 (0.000306)\n",
      "126. features 304 (0.000294)\n",
      "127. features 256 (0.000293)\n",
      "128. features 245 (0.000290)\n",
      "129. features 276 (0.000290)\n",
      "130. features 233 (0.000289)\n",
      "131. features 326 (0.000288)\n",
      "132. features 303 (0.000282)\n",
      "133. features 101 (0.000278)\n",
      "134. features 296 (0.000277)\n",
      "135. features 479 (0.000276)\n",
      "136. features 297 (0.000275)\n",
      "137. features 261 (0.000271)\n",
      "138. features 80 (0.000270)\n",
      "139. features 89 (0.000262)\n",
      "140. features 408 (0.000261)\n",
      "141. features 96 (0.000252)\n",
      "142. features 251 (0.000247)\n",
      "143. features 383 (0.000233)\n",
      "144. features 26 (0.000230)\n",
      "145. features 257 (0.000227)\n",
      "146. features 209 (0.000224)\n",
      "147. features 107 (0.000223)\n",
      "148. features 17 (0.000220)\n",
      "149. features 378 (0.000219)\n",
      "150. features 44 (0.000219)\n",
      "151. features 317 (0.000219)\n",
      "152. features 232 (0.000217)\n",
      "153. features 221 (0.000213)\n",
      "154. features 223 (0.000213)\n",
      "155. features 263 (0.000207)\n",
      "156. features 270 (0.000206)\n",
      "157. features 45 (0.000199)\n",
      "158. features 229 (0.000196)\n",
      "159. features 97 (0.000193)\n",
      "160. features 295 (0.000192)\n",
      "161. features 385 (0.000189)\n",
      "162. features 409 (0.000189)\n",
      "163. features 252 (0.000187)\n",
      "164. features 210 (0.000184)\n",
      "165. features 294 (0.000182)\n",
      "166. features 282 (0.000181)\n",
      "167. features 299 (0.000181)\n",
      "168. features 237 (0.000177)\n",
      "169. features 278 (0.000175)\n",
      "170. features 286 (0.000174)\n",
      "171. features 241 (0.000174)\n",
      "172. features 21 (0.000171)\n",
      "173. features 281 (0.000170)\n",
      "174. features 213 (0.000169)\n",
      "175. features 404 (0.000166)\n",
      "176. features 279 (0.000166)\n",
      "177. features 24 (0.000164)\n",
      "178. features 253 (0.000157)\n",
      "179. features 214 (0.000154)\n",
      "180. features 359 (0.000153)\n",
      "181. features 220 (0.000150)\n",
      "182. features 230 (0.000142)\n",
      "183. features 240 (0.000142)\n",
      "184. features 284 (0.000141)\n",
      "185. features 259 (0.000141)\n",
      "186. features 48 (0.000140)\n",
      "187. features 207 (0.000139)\n",
      "188. features 379 (0.000138)\n",
      "189. features 248 (0.000136)\n",
      "190. features 298 (0.000132)\n",
      "191. features 400 (0.000130)\n",
      "192. features 433 (0.000130)\n",
      "193. features 275 (0.000129)\n",
      "194. features 426 (0.000125)\n",
      "195. features 425 (0.000125)\n",
      "196. features 283 (0.000124)\n",
      "197. features 267 (0.000124)\n",
      "198. features 246 (0.000124)\n",
      "199. features 396 (0.000121)\n",
      "200. features 61 (0.000121)\n",
      "201. features 243 (0.000119)\n",
      "202. features 206 (0.000117)\n",
      "203. features 46 (0.000117)\n",
      "204. features 395 (0.000115)\n",
      "205. features 52 (0.000115)\n",
      "206. features 227 (0.000114)\n",
      "207. features 348 (0.000112)\n",
      "208. features 108 (0.000108)\n",
      "209. features 277 (0.000107)\n",
      "210. features 62 (0.000106)\n",
      "211. features 337 (0.000106)\n",
      "212. features 25 (0.000105)\n",
      "213. features 315 (0.000105)\n",
      "214. features 234 (0.000105)\n",
      "215. features 288 (0.000105)\n",
      "216. features 377 (0.000104)\n",
      "217. features 258 (0.000104)\n",
      "218. features 289 (0.000104)\n",
      "219. features 47 (0.000099)\n",
      "220. features 72 (0.000099)\n",
      "221. features 85 (0.000097)\n",
      "222. features 339 (0.000097)\n",
      "223. features 51 (0.000096)\n",
      "224. features 91 (0.000095)\n",
      "225. features 266 (0.000093)\n",
      "226. features 402 (0.000093)\n",
      "227. features 43 (0.000093)\n",
      "228. features 351 (0.000092)\n",
      "229. features 23 (0.000089)\n",
      "230. features 50 (0.000088)\n",
      "231. features 393 (0.000087)\n",
      "232. features 313 (0.000086)\n",
      "233. features 109 (0.000086)\n",
      "234. features 218 (0.000086)\n",
      "235. features 344 (0.000085)\n",
      "236. features 331 (0.000084)\n",
      "237. features 341 (0.000084)\n",
      "238. features 58 (0.000084)\n",
      "239. features 387 (0.000083)\n",
      "240. features 401 (0.000083)\n",
      "241. features 231 (0.000081)\n",
      "242. features 95 (0.000081)\n",
      "243. features 376 (0.000081)\n",
      "244. features 371 (0.000080)\n",
      "245. features 224 (0.000080)\n",
      "246. features 340 (0.000080)\n",
      "247. features 336 (0.000080)\n",
      "248. features 360 (0.000080)\n",
      "249. features 367 (0.000079)\n",
      "250. features 291 (0.000079)\n",
      "251. features 217 (0.000079)\n",
      "252. features 372 (0.000077)\n",
      "253. features 392 (0.000077)\n",
      "254. features 59 (0.000077)\n",
      "255. features 33 (0.000077)\n",
      "256. features 328 (0.000076)\n",
      "257. features 60 (0.000073)\n",
      "258. features 361 (0.000072)\n",
      "259. features 338 (0.000070)\n",
      "260. features 273 (0.000069)\n",
      "261. features 176 (0.000068)\n",
      "262. features 405 (0.000068)\n",
      "263. features 349 (0.000067)\n",
      "264. features 185 (0.000066)\n",
      "265. features 564 (0.000066)\n",
      "266. features 175 (0.000065)\n",
      "267. features 369 (0.000064)\n",
      "268. features 22 (0.000063)\n",
      "269. features 373 (0.000063)\n",
      "270. features 125 (0.000063)\n",
      "271. features 309 (0.000061)\n",
      "272. features 390 (0.000061)\n",
      "273. features 432 (0.000060)\n",
      "274. features 84 (0.000060)\n",
      "275. features 347 (0.000059)\n",
      "276. features 280 (0.000059)\n",
      "277. features 119 (0.000058)\n",
      "278. features 320 (0.000058)\n",
      "279. features 306 (0.000058)\n",
      "280. features 350 (0.000057)\n",
      "281. features 364 (0.000057)\n",
      "282. features 352 (0.000057)\n",
      "283. features 308 (0.000056)\n",
      "284. features 310 (0.000056)\n",
      "285. features 228 (0.000056)\n",
      "286. features 403 (0.000055)\n",
      "287. features 135 (0.000055)\n",
      "288. features 429 (0.000055)\n",
      "289. features 238 (0.000053)\n",
      "290. features 394 (0.000052)\n",
      "291. features 197 (0.000052)\n",
      "292. features 386 (0.000052)\n",
      "293. features 370 (0.000052)\n",
      "294. features 74 (0.000051)\n",
      "295. features 322 (0.000051)\n",
      "296. features 391 (0.000051)\n",
      "297. features 287 (0.000050)\n",
      "298. features 318 (0.000049)\n",
      "299. features 177 (0.000049)\n",
      "300. features 374 (0.000047)\n",
      "301. features 398 (0.000047)\n",
      "302. features 380 (0.000045)\n",
      "303. features 249 (0.000045)\n",
      "304. features 186 (0.000045)\n",
      "305. features 342 (0.000045)\n",
      "306. features 355 (0.000045)\n",
      "307. features 375 (0.000044)\n",
      "308. features 225 (0.000044)\n",
      "309. features 130 (0.000043)\n",
      "310. features 399 (0.000043)\n",
      "311. features 137 (0.000042)\n",
      "312. features 332 (0.000041)\n",
      "313. features 183 (0.000041)\n",
      "314. features 358 (0.000040)\n",
      "315. features 363 (0.000040)\n",
      "316. features 168 (0.000040)\n",
      "317. features 356 (0.000039)\n",
      "318. features 345 (0.000039)\n",
      "319. features 362 (0.000039)\n",
      "320. features 365 (0.000039)\n",
      "321. features 334 (0.000038)\n",
      "322. features 156 (0.000038)\n",
      "323. features 333 (0.000037)\n",
      "324. features 164 (0.000037)\n",
      "325. features 366 (0.000037)\n",
      "326. features 167 (0.000036)\n",
      "327. features 354 (0.000036)\n",
      "328. features 384 (0.000036)\n",
      "329. features 199 (0.000035)\n",
      "330. features 202 (0.000034)\n",
      "331. features 325 (0.000034)\n",
      "332. features 427 (0.000034)\n",
      "333. features 323 (0.000033)\n",
      "334. features 250 (0.000033)\n",
      "335. features 330 (0.000033)\n",
      "336. features 368 (0.000032)\n",
      "337. features 128 (0.000031)\n",
      "338. features 188 (0.000031)\n",
      "339. features 162 (0.000030)\n",
      "340. features 133 (0.000030)\n",
      "341. features 327 (0.000030)\n",
      "342. features 397 (0.000030)\n",
      "343. features 171 (0.000030)\n",
      "344. features 159 (0.000030)\n",
      "345. features 523 (0.000029)\n",
      "346. features 382 (0.000029)\n",
      "347. features 144 (0.000029)\n",
      "348. features 486 (0.000029)\n",
      "349. features 123 (0.000027)\n",
      "350. features 124 (0.000026)\n",
      "351. features 389 (0.000026)\n",
      "352. features 165 (0.000026)\n",
      "353. features 343 (0.000025)\n",
      "354. features 136 (0.000025)\n",
      "355. features 170 (0.000025)\n",
      "356. features 312 (0.000025)\n",
      "357. features 335 (0.000025)\n",
      "358. features 321 (0.000025)\n",
      "359. features 134 (0.000024)\n",
      "360. features 529 (0.000023)\n",
      "361. features 430 (0.000022)\n",
      "362. features 146 (0.000022)\n",
      "363. features 205 (0.000022)\n",
      "364. features 329 (0.000021)\n",
      "365. features 324 (0.000021)\n",
      "366. features 129 (0.000021)\n",
      "367. features 192 (0.000021)\n",
      "368. features 122 (0.000020)\n",
      "369. features 198 (0.000020)\n",
      "370. features 424 (0.000020)\n",
      "371. features 153 (0.000019)\n",
      "372. features 155 (0.000019)\n",
      "373. features 154 (0.000019)\n",
      "374. features 127 (0.000019)\n",
      "375. features 346 (0.000018)\n",
      "376. features 467 (0.000018)\n",
      "377. features 203 (0.000018)\n",
      "378. features 357 (0.000017)\n",
      "379. features 150 (0.000017)\n",
      "380. features 422 (0.000017)\n",
      "381. features 117 (0.000016)\n",
      "382. features 190 (0.000016)\n",
      "383. features 193 (0.000016)\n",
      "384. features 423 (0.000016)\n",
      "385. features 112 (0.000016)\n",
      "386. features 126 (0.000015)\n",
      "387. features 149 (0.000015)\n",
      "388. features 431 (0.000015)\n",
      "389. features 381 (0.000015)\n",
      "390. features 200 (0.000015)\n",
      "391. features 169 (0.000015)\n",
      "392. features 113 (0.000014)\n",
      "393. features 172 (0.000014)\n",
      "394. features 138 (0.000014)\n",
      "395. features 307 (0.000013)\n",
      "396. features 158 (0.000012)\n",
      "397. features 141 (0.000011)\n",
      "398. features 120 (0.000011)\n",
      "399. features 194 (0.000011)\n",
      "400. features 143 (0.000011)\n",
      "401. features 179 (0.000011)\n",
      "402. features 196 (0.000011)\n",
      "403. features 178 (0.000011)\n",
      "404. features 174 (0.000011)\n",
      "405. features 189 (0.000010)\n",
      "406. features 184 (0.000009)\n",
      "407. features 469 (0.000009)\n",
      "408. features 314 (0.000009)\n",
      "409. features 161 (0.000009)\n",
      "410. features 163 (0.000009)\n",
      "411. features 195 (0.000009)\n",
      "412. features 454 (0.000009)\n",
      "413. features 157 (0.000008)\n",
      "414. features 152 (0.000008)\n",
      "415. features 114 (0.000008)\n",
      "416. features 181 (0.000008)\n",
      "417. features 118 (0.000008)\n",
      "418. features 132 (0.000008)\n",
      "419. features 131 (0.000007)\n",
      "420. features 204 (0.000007)\n",
      "421. features 116 (0.000007)\n",
      "422. features 140 (0.000007)\n",
      "423. features 145 (0.000007)\n",
      "424. features 106 (0.000006)\n",
      "425. features 115 (0.000006)\n",
      "426. features 148 (0.000006)\n",
      "427. features 187 (0.000006)\n",
      "428. features 500 (0.000005)\n",
      "429. features 191 (0.000005)\n",
      "430. features 166 (0.000005)\n",
      "431. features 139 (0.000005)\n",
      "432. features 560 (0.000005)\n",
      "433. features 545 (0.000005)\n",
      "434. features 151 (0.000004)\n",
      "435. features 201 (0.000004)\n",
      "436. features 180 (0.000004)\n",
      "437. features 147 (0.000004)\n",
      "438. features 319 (0.000004)\n",
      "439. features 497 (0.000004)\n",
      "440. features 420 (0.000003)\n",
      "441. features 440 (0.000003)\n",
      "442. features 530 (0.000002)\n",
      "443. features 441 (0.000002)\n",
      "444. features 142 (0.000002)\n",
      "445. features 110 (0.000002)\n",
      "446. features 111 (0.000002)\n",
      "447. features 160 (0.000001)\n",
      "448. features 121 (0.000001)\n",
      "449. features 528 (0.000000)\n",
      "450. features 537 (0.000000)\n",
      "451. features 512 (0.000000)\n",
      "452. features 531 (0.000000)\n",
      "453. features 532 (0.000000)\n",
      "454. features 510 (0.000000)\n",
      "455. features 533 (0.000000)\n",
      "456. features 534 (0.000000)\n",
      "457. features 535 (0.000000)\n",
      "458. features 509 (0.000000)\n",
      "459. features 511 (0.000000)\n",
      "460. features 514 (0.000000)\n",
      "461. features 513 (0.000000)\n",
      "462. features 527 (0.000000)\n",
      "463. features 518 (0.000000)\n",
      "464. features 526 (0.000000)\n",
      "465. features 515 (0.000000)\n",
      "466. features 525 (0.000000)\n",
      "467. features 524 (0.000000)\n",
      "468. features 522 (0.000000)\n",
      "469. features 516 (0.000000)\n",
      "470. features 517 (0.000000)\n",
      "471. features 521 (0.000000)\n",
      "472. features 520 (0.000000)\n",
      "473. features 519 (0.000000)\n",
      "474. features 536 (0.000000)\n",
      "475. features 567 (0.000000)\n",
      "476. features 538 (0.000000)\n",
      "477. features 561 (0.000000)\n",
      "478. features 550 (0.000000)\n",
      "479. features 549 (0.000000)\n",
      "480. features 548 (0.000000)\n",
      "481. features 507 (0.000000)\n",
      "482. features 551 (0.000000)\n",
      "483. features 552 (0.000000)\n",
      "484. features 553 (0.000000)\n",
      "485. features 554 (0.000000)\n",
      "486. features 555 (0.000000)\n",
      "487. features 556 (0.000000)\n",
      "488. features 557 (0.000000)\n",
      "489. features 558 (0.000000)\n",
      "490. features 559 (0.000000)\n",
      "491. features 547 (0.000000)\n",
      "492. features 562 (0.000000)\n",
      "493. features 539 (0.000000)\n",
      "494. features 563 (0.000000)\n",
      "495. features 565 (0.000000)\n",
      "496. features 566 (0.000000)\n",
      "497. features 546 (0.000000)\n",
      "498. features 568 (0.000000)\n",
      "499. features 569 (0.000000)\n",
      "500. features 570 (0.000000)\n",
      "501. features 571 (0.000000)\n",
      "502. features 544 (0.000000)\n",
      "503. features 543 (0.000000)\n",
      "504. features 542 (0.000000)\n",
      "505. features 541 (0.000000)\n",
      "506. features 572 (0.000000)\n",
      "507. features 540 (0.000000)\n",
      "508. features 508 (0.000000)\n",
      "509. features 573 (0.000000)\n",
      "510. features 506 (0.000000)\n",
      "511. features 451 (0.000000)\n",
      "512. features 465 (0.000000)\n",
      "513. features 464 (0.000000)\n",
      "514. features 463 (0.000000)\n",
      "515. features 462 (0.000000)\n",
      "516. features 461 (0.000000)\n",
      "517. features 460 (0.000000)\n",
      "518. features 459 (0.000000)\n",
      "519. features 458 (0.000000)\n",
      "520. features 457 (0.000000)\n",
      "521. features 182 (0.000000)\n",
      "522. features 456 (0.000000)\n",
      "523. features 455 (0.000000)\n",
      "524. features 453 (0.000000)\n",
      "525. features 452 (0.000000)\n",
      "526. features 450 (0.000000)\n",
      "527. features 505 (0.000000)\n",
      "528. features 449 (0.000000)\n",
      "529. features 448 (0.000000)\n",
      "530. features 447 (0.000000)\n",
      "531. features 446 (0.000000)\n",
      "532. features 445 (0.000000)\n",
      "533. features 444 (0.000000)\n",
      "534. features 443 (0.000000)\n",
      "535. features 442 (0.000000)\n",
      "536. features 439 (0.000000)\n",
      "537. features 438 (0.000000)\n",
      "538. features 437 (0.000000)\n",
      "539. features 436 (0.000000)\n",
      "540. features 435 (0.000000)\n",
      "541. features 434 (0.000000)\n",
      "542. features 466 (0.000000)\n",
      "543. features 468 (0.000000)\n",
      "544. features 470 (0.000000)\n",
      "545. features 471 (0.000000)\n",
      "546. features 504 (0.000000)\n",
      "547. features 503 (0.000000)\n",
      "548. features 502 (0.000000)\n",
      "549. features 501 (0.000000)\n",
      "550. features 499 (0.000000)\n",
      "551. features 498 (0.000000)\n",
      "552. features 496 (0.000000)\n",
      "553. features 495 (0.000000)\n",
      "554. features 494 (0.000000)\n",
      "555. features 493 (0.000000)\n",
      "556. features 492 (0.000000)\n",
      "557. features 491 (0.000000)\n",
      "558. features 490 (0.000000)\n",
      "559. features 489 (0.000000)\n",
      "560. features 488 (0.000000)\n",
      "561. features 487 (0.000000)\n",
      "562. features 485 (0.000000)\n",
      "563. features 484 (0.000000)\n",
      "564. features 483 (0.000000)\n",
      "565. features 482 (0.000000)\n",
      "566. features 481 (0.000000)\n",
      "567. features 480 (0.000000)\n",
      "568. features 478 (0.000000)\n",
      "569. features 477 (0.000000)\n",
      "570. features 476 (0.000000)\n",
      "571. features 475 (0.000000)\n",
      "572. features 474 (0.000000)\n",
      "573. features 473 (0.000000)\n",
      "574. features 472 (0.000000)\n",
      "575. features 574 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = best_clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_clf.estimators_], axis = 0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print('Feature Ranking:')\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "\tprint('%d. features %d (%f)'% (f+1, indices[f], importances[indices[f]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "77238a471535228e8cd55a3ca9e771a69c6c0bc66c44a56c972f9554a4042742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
